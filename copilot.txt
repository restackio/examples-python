
# Repository structure

A Restack backend application should be structured as follows:

- src/
    - client.py
    - functions/
        - __init__.py
        - function.py
    - workflows/
        - __init__.py
        - workflow.py
    - services.py
    - schedule_workflow.py
    - pyproject.toml
    - env.example
    - README.md
    - Dockerfile
    - restack_up.py

All these files are mandatory.

### File: ./openai_greet/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass

@dataclass
class InputParams:
    name: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-OpenaiGreetWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="OpenaiGreetWorkflow",
        workflow_id=workflow_id,
        input=InputParams(name="Restack AI SDK User")
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./openai_greet/src/services.py ###
import asyncio
from src.functions.function import openai_greet
from src.client import client
from src.workflows.openai_greet import OpenaiGreetWorkflow
async def main():

    await client.start_service(
        workflows= [OpenaiGreetWorkflow],
        functions= [openai_greet],
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./openai_greet/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./openai_greet/src/workflows/openai_greet.py ###
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.function import openai_greet, FunctionInputParams

@dataclass
class WorkflowInputParams:
    name: str

@workflow.defn()
class OpenaiGreetWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("OpenaiGreetWorkflow started", input=input)
        user_content = f"Greet this person {input.name}"


        greet_message = await workflow.step(
            openai_greet,
            FunctionInputParams(
                user_content=user_content,
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )
        log.info("OpenaiGreetWorkflow completed", greet_message=greet_message)
        return greet_message


### File: ./openai_greet/src/functions/function.py ###
from restack_ai.function import function, log
from openai import OpenAI
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class FunctionInputParams:
    user_content: str
    system_content: str | None = None
    model: str | None = None

@function.defn()
async def openai_greet(input: FunctionInputParams) -> str:
    try:
        log.info("openai_greet function started", input=input)
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

        messages = []
        if input.system_content:
            messages.append({"role": "system", "content": input.system_content})
        messages.append({"role": "user", "content": input.user_content})

        response = client.chat.completions.create(
            model=input.model or "gpt-4o-mini",
            messages=messages,
            response_format={
                "json_schema": {
                    "name": "greet",
                    "description": "Greet a person",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "message": {"type": "string"}
                        },
                        "required": ["message"]
                    }
                },
                "type": "json_schema",
            },
        )
        log.info("openai_greet function completed", response=response)
        return response.choices[0].message.content
    except Exception as e:
        log.error("openai_greet function failed", error=e)
        raise e


### File: ./encryption/schedule_workflow.py ###
import asyncio
import time
from src.client import client
async def main():

    workflow_id = f"{int(time.time() * 1000)}-EncryptedWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="EncryptedWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./encryption/src/services.py ###
import asyncio
from src.functions.function import welcome
from src.client import client
from src.workflows.workflow import EncryptedWorkflow

async def main():

    await client.start_service(
        workflows= [EncryptedWorkflow],
        functions= [welcome]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./encryption/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from restack_ai.security import converter
import dataclasses
from .codec import EncryptionCodec

connection_options = CloudConnectionOptions(
    engine_id=os.getenv("RESTACK_ENGINE_ID"),
    api_key=os.getenv("RESTACK_ENGINE_API_KEY"),
    address=os.getenv("RESTACK_ENGINE_ADDRESS"),
    data_converter=dataclasses.replace(converter.default(), payload_codec=EncryptionCodec())
)
client = Restack(connection_options)


### File: ./encryption/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome

@workflow.defn()
class EncryptedWorkflow:
    @workflow.run
    async def run(self):
        log.info("EncryptedWorkflow started")
        result = await workflow.step(welcome, input="world", start_to_close_timeout=timedelta(seconds=120))
        log.info("EncryptedWorkflow completed", result=result)
        return result




### File: ./encryption/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn()
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: ./encryption/src/codec.py ###
import os
from typing import Iterable, List

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from restack_ai.security import Payload, PayloadCodec

default_key = b"test-key-test-key-test-key-test!"
default_key_id = "test-key-id"


class EncryptionCodec(PayloadCodec):
    def __init__(self, key_id: str = default_key_id, key: bytes = default_key) -> None:
        super().__init__()
        self.key_id = key_id
        # We are using direct AESGCM to be compatible with samples from
        # TypeScript and Go. Pure Python samples may prefer the higher-level,
        # safer APIs.
        self.encryptor = AESGCM(key)

    async def encode(self, payloads: Iterable[Payload]) -> List[Payload]:
        # We blindly encode all payloads with the key and set the metadata
        # saying which key we used
        return [
            Payload(
                metadata={
                    "encoding": b"binary/encrypted",
                    "encryption-key-id": self.key_id.encode(),
                },
                data=self.encrypt(p.SerializeToString()),
            )
            for p in payloads
        ]

    async def decode(self, payloads: Iterable[Payload]) -> List[Payload]:
        ret: List[Payload] = []
        for p in payloads:
            # Ignore ones w/out our expected encoding
            if p.metadata.get("encoding", b"").decode() != "binary/encrypted":
                ret.append(p)
                continue
            # Confirm our key ID is the same
            key_id = p.metadata.get("encryption-key-id", b"").decode()
            if key_id != self.key_id:
                raise ValueError(
                    f"Unrecognized key ID {key_id}. Current key ID is {self.key_id}."
                )
            # Decrypt and append
            ret.append(Payload.FromString(self.decrypt(p.data)))
        return ret

    def encrypt(self, data: bytes) -> bytes:
        nonce = os.urandom(12)
        return nonce + self.encryptor.encrypt(nonce, data, None)

    def decrypt(self, data: bytes) -> bytes:
        return self.encryptor.decrypt(data[:12], data[12:], None)

### File: ./encryption/src/codec_server.py ###
from functools import partial
from typing import Awaitable, Callable, Iterable, List

from aiohttp import hdrs, web
from google.protobuf import json_format
from restack_ai.security import Payload, Payloads

from .codec import EncryptionCodec


def build_codec_server() -> web.Application:
    # Cors handler
    async def cors_options(req: web.Request) -> web.Response:
        resp = web.Response()
        if req.headers.get(hdrs.ORIGIN) == "http://localhost:8233":
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_ORIGIN] = "http://localhost:8233"
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_METHODS] = "POST"
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_HEADERS] = "content-type,x-namespace"
        return resp

    # General purpose payloads-to-payloads
    async def apply(
        fn: Callable[[Iterable[Payload]], Awaitable[List[Payload]]], req: web.Request
    ) -> web.Response:
        # Read payloads as JSON
        assert req.content_type == "application/json"
        payloads = json_format.Parse(await req.read(), Payloads())

        # Apply
        payloads = Payloads(payloads=await fn(payloads.payloads))

        # Apply CORS and return JSON
        resp = await cors_options(req)
        resp.content_type = "application/json"
        resp.text = json_format.MessageToJson(payloads)
        return resp

    # Build app
    codec = EncryptionCodec()
    app = web.Application()
    app.add_routes(
        [
            web.post("/encode", partial(apply, codec.encode)),
            web.post("/decode", partial(apply, codec.decode)),
            web.options("/decode", cors_options),
        ]
    )
    return app

def run_codec_server():
    web.run_app(build_codec_server(), host="127.0.0.1", port=8081)

### File: ./custom_llm_gemini/src/llm.py ###
from __future__ import annotations
import asyncio
import json
import os
import time
import traceback

from dotenv import load_dotenv
from flask import Flask, Response, request, stream_with_context
import google.generativeai as genai
from flask_cors import CORS

load_dotenv()

app = Flask(__name__)
CORS(app)

def run_async(coro):
    loop = asyncio.new_event_loop()
    return loop.run_until_complete(coro)


@app.route("/chat/completions", methods=["POST"])
def chat_completion():
    try:
        api_key = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=api_key)
        
        # Log that a request has been received
        print("Received a POST request at /chat/completions")

        # Log request headers
        print("Request headers:", request.headers)
        
        start_time = time.perf_counter()
        data = request.json
        messages = data.get("messages", [])
        print("Received messages:", messages)

        # Log request headers
        print("Request headers:", request.headers)

        # transform messages to the format expected by the gemini api
        content_objects = []

        for message in messages:
            part = genai.protos.Part(text=message['content'])
            role = "model" if message['role'] in ["system", "assistant"] else message['role']
            content = genai.protos.Content(parts=[part], role=role)
            content_objects.append(content)

        # Start the completion stream

        completion_stream = genai.GenerativeModel('models/gemini-1.5-flash').generate_content(
            contents=content_objects,
            stream=True
        )

        print(f"Time taken to start streaming: {time.perf_counter() - start_time}")

        def generate():
            for chunk in completion_stream:
                print(chunk)
                # Access the candidates directly from the response
                for candidate in chunk._result.candidates:
                    content = candidate.content.parts[0].text
                    print(content)
                    yield f"data: {json.dumps({'choices': [{'delta': {'content': content}}]})}\n\n"
            yield "data: [DONE]\n\n"

        return Response(stream_with_context(generate()), content_type="text/plain")

    except Exception as e:
        print(f"CHATBOT_STEP: {traceback.format_exc()}")
        return Response(str(e), content_type="text/plain", status=500)


@app.route("/test", methods=["GET"])
def test_route():
    print("Test route accessed")
    return "Test route is working", 200
    
if __name__ == "__main__":
    app.run(debug=True)

def run_llm():
    app.run(debug=True, host='0.0.0.0', port=1337)


### File: ./fastapi_gemini_feedback/src/services.py ###
import asyncio
from src.client import client
from src.functions.function import gemini_generate
from src.workflows.gemini_generate_content import GeminiGenerateWorkflow
async def main():
    await client.start_service(
        workflows= [GeminiGenerateWorkflow],
        functions= [gemini_generate]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./fastapi_gemini_feedback/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)

client = Restack(connection_options)

### File: ./fastapi_gemini_feedback/src/workflows/gemini_generate_content.py ###
from restack_ai.workflow import workflow, import_functions, log
from pydantic import BaseModel
from datetime import timedelta
from dataclasses import dataclass

with import_functions():
    from src.functions.function import gemini_generate, FunctionInputParams

@dataclass
class Feedback:
    feedback: str

@dataclass
class End:
    end: bool

class WorkflowInputParams(BaseModel):
    user_content: str

@workflow.defn()
class GeminiGenerateWorkflow:
    def __init__(self) -> None:
        self.end_workflow = False
        self.feedbacks = []
        self.user_content = ""
    @workflow.event
    async def event_feedback(self, feedback: Feedback) -> Feedback:
        log.info(f"Received feedback: {feedback.feedback}")
        self.feedbacks.append(feedback.feedback)
        return await workflow.step(gemini_generate, FunctionInputParams(user_content=f"{self.user_content}. Take into account all feedbacks: {self.feedbacks}"), start_to_close_timeout=timedelta(seconds=120))
    
    @workflow.event
    async def event_end(self, end: End) -> End:
        log.info(f"Received end")
        self.end_workflow = True
        return end
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        self.user_content = input.user_content
        await workflow.step(gemini_generate, FunctionInputParams(user_content=input.user_content), start_to_close_timeout=timedelta(seconds=120))
        await workflow.condition(
            lambda: self.end_workflow
        )
        return


### File: ./fastapi_gemini_feedback/src/app.py ###
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from restack_ai import Restack
import time

import uvicorn

app = FastAPI()



@app.get("/")
async def home():
    return {"message": "Welcome to the FastAPI App!"}

@app.get("/test")
async def test_route():
    return {"message": "This is a test route"}

class InputParams(BaseModel):
    user_content: str

@app.post("/api/schedule")
async def schedule_workflow(data: InputParams):
    
    print(data)

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GeminiGenerateWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="GeminiGenerateWorkflow",
        workflow_id=workflow_id,
        input=data
    )

    print(f"Scheduled workflow with run_id: {run_id}")

    return {
        "workflow_id": workflow_id,
        "run_id": run_id
    }

class FeedbackParams(BaseModel):
    workflow_id: str
    run_id: str
    feedback: str

@app.post("/api/event/feedback")
async def send_event_feedback(data: FeedbackParams):
    client = Restack()

    print(f"event feedback: {data}")

    await client.send_workflow_event(
        workflow_id=data.workflow_id,
        run_id=data.run_id,
        event_name="event_feedback",
        event_input={"feedback": data.feedback}
    )
    return

class EndParams(BaseModel):
    workflow_id: str
    run_id: str

@app.post("/api/event/end")
async def send_event_end(data: EndParams):
    client = Restack()

    await client.send_workflow_event(
        workflow_id=data.workflow_id,
        run_id=data.run_id,
        event_name="event_end"
    )
    return

def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=5001, reload=True)

if __name__ == '__main__':
    run_app()

### File: ./fastapi_gemini_feedback/src/functions/function.py ###
import os
from dataclasses import dataclass
from restack_ai.function import function, log
import google.generativeai as genai

@dataclass
class FunctionInputParams:
    user_content: str

@function.defn()
async def gemini_generate(input: FunctionInputParams) -> str:
    try:
        log.info("gemini_generate function started", input=input)
        genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-1.5-flash")

        response = model.generate_content(input.user_content)
        log.info("gemini_generate function completed", response=response.text)
        return response.text
    except Exception as e:
        log.error("gemini_generate function failed", error=e)
        raise e


### File: ./defense_quickstart_news_scraper_summarizer/frontend.py ###
import streamlit as st
import requests

# Set page title and header
st.title("Defense Hackathon Quickstart: War News Scraper & Summarizer")

# Create text area for user input with session state
if "user_input" not in st.session_state:
    st.session_state.user_input = ""

url = st.text_input("Rss feed url", key="url", value="https://www.pravda.com.ua/rss/")
count = st.number_input("Number of results", key="count", value=2)
# Initialize response history in session state
if "response_history" not in st.session_state:
    st.session_state.response_history = []

# Create button to send request
if st.button("Search"):
    if url:
        try:
            with st.spinner('Searching...'):
                # Make POST request to FastAPI backend
                response = requests.post(
                    "http://localhost:8000/api/schedule",
                    json={"url": url, "count": count}
                )
                
                if response.status_code == 200:
                    st.success("Response received!")
                    # Add the new response to history with the original prompt
                    st.session_state.response_history.append({
                        "url": url,
                        "count": count,
                        "response": response.json()["result"]
                    })
                else:
                    st.error(f"Error: {response.status_code}")
                    
        except requests.exceptions.ConnectionError:
            st.error("Failed to connect to the server. Make sure the FastAPI server is running.")
    else:
        st.warning("Please enter a prompt before submitting.")

# Display response history
if st.session_state.response_history:
    st.subheader("Response History")
    for i, item in enumerate(st.session_state.response_history, 1):
        st.markdown(f"**Url {i}:** {item['url']}")
        st.markdown(f"**Count {i}:** {item['count']}")
        st.markdown(f"**Response {i}:** {item['response']}")
        st.markdown("---")


### File: ./defense_quickstart_news_scraper_summarizer/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    engine = {
        'name': 'restack_engine',
        'image': 'ghcr.io/restackio/restack:main',
        'portMapping': [
            {
                'port': 5233,
                'path': '/',
                'name': 'engine-frontend',
            },
            {
                'port': 6233,
                'path': '/api',
                'name': 'engine-api',
            }
        ],
        'environmentVariables': [
          {
              'name': 'RESTACK_ENGINE_ID',
              'value': os.getenv('RESTACK_ENGINE_ID'),
          },
          {
              'name': 'RESTACK_ENGINE_ADDRESS',
              'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
          },
          {
              'name': 'RESTACK_ENGINE_API_KEY',
              'value': os.getenv('RESTACK_ENGINE_API_KEY'),
          },
        ],
    }

    # Define the application configuration
    app = {
        'name': 'defense_quickstart_news_scraper_summarizer',
        'dockerFilePath': '/defense_quickstart_news_scraper_summarizer/Dockerfile',
        'dockerBuildContext': './defense_quickstart_news_scraper_summarizer/',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
            {
                'name': 'OPENBABYLON_API_URL',
                'value': os.getenv('OPENBABYLON_API_URL'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'defense_quickstart_news_scraper_summarizer',
        'previewEnabled': False,
        'applications': [engine,app],
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./defense_quickstart_news_scraper_summarizer/src/services.py ###
import asyncio
from src.client import client
from src.functions.llm.chat import llm_chat
from src.functions.rss.pull import rss_pull
from src.workflows.workflow import RssWorkflow
from src.functions.crawl.website import crawl_website
from src.functions.helper.split_text import split_text
from restack_ai.restack import ServiceOptions

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[RssWorkflow],
            functions=[rss_pull, crawl_website, split_text]
        ),
        client.start_service(
            functions=[llm_chat],
            task_queue="llm_chat",
            options=ServiceOptions(
                rate_limit=1,
                max_concurrent_function_runs=1
            )
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./defense_quickstart_news_scraper_summarizer/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)



### File: ./defense_quickstart_news_scraper_summarizer/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.rss.pull import rss_pull
    from src.functions.crawl.website import crawl_website
    from src.functions.helper.split_text import split_text
    from src.functions.llm.chat import llm_chat, FunctionInputParams
    from src.functions.rss.schema import RssInput

@workflow.defn()
class RssWorkflow:
    @workflow.run
    async def run(self, input: dict):

        url = input["url"]
        count = input["count"]
        rss_results = await workflow.step(rss_pull, RssInput(url=url, count=count), start_to_close_timeout=timedelta(seconds=10))
        urls = [item['link'] for item in rss_results if 'link' in item]
        titles = [item['title'] for item in rss_results if 'title' in item]


        crawled_contents = []
        for url in urls:
            log.info("rss_result", extra={"url": url})
            if url:
                try:
                    content = await workflow.step(crawl_website, url, start_to_close_timeout=timedelta(seconds=30))
                    split_content = await workflow.step(split_text, f"{titles[urls.index(url)]}\n\n{content}", start_to_close_timeout=timedelta(seconds=30))
                    crawled_contents.append(split_content)
                except Exception as e:
                    log.error(f"Failed to crawl {url}: {str(e)}")
                    continue
        summaries = []
        for split_content in crawled_contents:
            for content in split_content:
                user_prompt = f"Provide a translation of the news article. Translate the following content to English: {content}"
                translation = await workflow.step(llm_chat, FunctionInputParams(user_prompt=user_prompt), task_queue="llm_chat",start_to_close_timeout=timedelta(seconds=120))

                user_prompt = f"Provide a summary of the news found on rss feed. Summarize the following content: {translation} in maxium 1 sentence with no more than 20 words"
                summary = await workflow.step(llm_chat, FunctionInputParams(user_prompt=user_prompt), task_queue="llm_chat",start_to_close_timeout=timedelta(seconds=120))
                summaries.append(summary)

        user_prompt = f"Make a daily digest of all the news and tell me what is the most important news. Here are the summaries of the articles: {summaries}."

        return await workflow.step(llm_chat, FunctionInputParams(user_prompt=user_prompt), task_queue="llm_chat", start_to_close_timeout=timedelta(seconds=120))

### File: ./defense_quickstart_news_scraper_summarizer/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dataclasses import dataclass
from src.client import client
import time
import uvicorn


# Define request model
@dataclass
class QueryRequest:
    url: str
    count: int

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the Quickstart: War News Scraper & Summarizer example!"

@app.post("/api/schedule")
async def schedule_workflow(request: QueryRequest):
    try:

        workflow_id = f"{int(time.time() * 1000)}-rss_workflow"
        
        runId = await client.schedule_workflow(
            workflow_name="RssWorkflow",
            workflow_id=workflow_id,
            input={"url": request.url, "count": request.count}
        )
        print("Scheduled workflow", runId)
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {
            "result": result,
            "workflow_id": workflow_id,
            "run_id": runId
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# Remove Flask-specific run code since FastAPI uses uvicorn
def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./defense_quickstart_news_scraper_summarizer/src/functions/llm/chat.py ###
from restack_ai.function import function, log
from openai import OpenAI
from dataclasses import dataclass
import os

@dataclass
class ResponseFormat:
    name: str
    description: str
    schema: dict

@dataclass
class FunctionInputParams:
    user_prompt: str
    model: str | None = None

@function.defn()
async def llm_chat(input: FunctionInputParams) -> str:
    try:
        log.info("llm_chat function started", input=input)

        openbabylon_url = os.environ.get("OPENBABYLON_API_URL")
        log.info("openbabylon_url", openbabylon_url=openbabylon_url)

        client = OpenAI(api_key='openbabylon',base_url=os.environ.get("OPENBABYLON_API_URL"))

        messages = []
        if input.user_prompt:
            messages.append({"role": "user", "content": input.user_prompt})
        
        response = client.chat.completions.create(
            model="orpo-mistral-v0.3-ua-tokV2-focus-10B-low-lr-1epoch-aux-merged-1ep",
            messages=messages,
        )
        log.info("llm_chat function completed", response=response)
        return response.choices[0].message.content
    except Exception as e:
        log.error("llm_chat function failed", error=e)
        raise e

### File: ./defense_quickstart_news_scraper_summarizer/src/functions/rss/pull.py ###
import requests
import xml.etree.ElementTree as ET
from restack_ai.function import function, log

from .schema import RssInput

@function.defn()
async def rss_pull(input:RssInput):
    try:
        # Fetch the RSS feed
        response = requests.get(input.url)
        response.raise_for_status()  # Raise an error for bad responses

        # Parse the RSS feed
        root = ET.fromstring(response.content)
        items = []
        for item in root.findall(".//item"):
            title = item.find("title").text
            link = item.find("link").text
            description = item.find("description").text
            category = item.find("category").text if item.find("category") is not None else None
            creator = item.find("{http://purl.org/dc/elements/1.1/}creator").text if item.find("{http://purl.org/dc/elements/1.1/}creator") is not None else None
            pub_date = item.find("pubDate").text if item.find("pubDate") is not None else None
            content_encoded = item.find("{http://purl.org/rss/1.0/modules/content/}encoded").text if item.find("{http://purl.org/rss/1.0/modules/content/}encoded") is not None else None

            items.append({
                "title": title,
                "link": link,
                "description": description,
                "category": category,
                "creator": creator,
                "pub_date": pub_date,
                "content_encoded": content_encoded
            })

        # Limit the number of items based on input.count
        max_count = input.count if input.count is not None else len(items)
        items = items[:max_count]

        log.info("rss_pull", extra={"data": items})
        return items
    except Exception as error:
        log.error("rss_pull function failed", error=error)
        raise error

### File: ./defense_quickstart_news_scraper_summarizer/src/functions/rss/schema.py ###
from pydantic import BaseModel, Field

class RssInput(BaseModel):
    url: str = Field(default=None, description="The url to get rss from")
    count: int | None = Field(default=None, description="The number of results to return")

### File: ./defense_quickstart_news_scraper_summarizer/src/functions/crawl/website.py ###
from restack_ai.function import function, log
import requests
from bs4 import BeautifulSoup

@function.defn()
async def crawl_website(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad responses

        # Parse the content with BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract the text content from the page
        content = soup.get_text(separator=' ', strip=True)

        log.info("crawl_website", extra={"content": content})

        return content

    except requests.exceptions.RequestException as e:
        # Handle any exceptions that occur during the request
        log.error("crawl_website function failed", error=e)
        raise e


### File: ./defense_quickstart_news_scraper_summarizer/src/functions/helper/split_text.py ###
from restack_ai.function import function

@function.defn()
async def split_text(text: str, average_token_per_character: int = 3, max_tokens: int = 4096) -> list:
    chunks = []
    current_chunk = []
    current_length = 0

    for char in text:
        current_chunk.append(char)
        current_length += average_token_per_character

        if current_length >= max_tokens:
            chunks.append(''.join(current_chunk))
            current_chunk = []
            current_length = 0

    if current_chunk:
        chunks.append(''.join(current_chunk))

    return chunks

### File: ./streamlit/main.py ###
import streamlit as st
import asyncio
from restack_ai import Restack
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)

# Function to trigger a workflow
async def trigger_workflow(workflow_name, workflow_id, input_data):

    try:
        client = Restack()
        run_id = await client.schedule_workflow(
          workflow_name= workflow_name,
          workflow_id= workflow_id,
          input= input_data,
        )

        result = await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
        )
        print(result)

        return result
    except Exception as e:
        st.error(f"Failed to trigger workflow: {str(e)}")
        return None

# Streamlit UI
st.title("Trigger Restack Workflow")

workflow_name = st.text_input("Workflow Name")
workflow_id = st.text_input("Workflow ID")
input_data = st.text_area("Input Data (JSON format)")

if st.button("Trigger Workflow"):
    if not workflow_name or not workflow_id:
        st.error("Workflow name and ID are required.")
    else:
        input_dict = eval(input_data) if input_data else {}
        run_id = asyncio.run(trigger_workflow(workflow_name, workflow_id, input_dict))
        
        # Log the input data
        logging.info(f"Triggered workflow with input: {input_dict}")
        
        if run_id:
            st.success(f"Workflow triggered successfully with runId: {run_id}")


### File: ./defense_quickstart_denoise/frontend.py ###
import streamlit as st
import requests
import tempfile
from pathlib import Path

st.title("Defense Hackathon Quickstart: War Audio Noise Removal")

tmp_dir = Path(tempfile.gettempdir()) / 'streamlit_uploads'
tmp_dir.mkdir(exist_ok=True)

def get_file_path(uploaded_file):
    temp_path = tmp_dir / uploaded_file.name
    temp_path.write_bytes(uploaded_file.getvalue())
    return str(temp_path.absolute())

uploaded_files = st.file_uploader("Choose files", accept_multiple_files=True)

if uploaded_files:
    file_data_list = []
    file_paths = []

    for uploaded_file in uploaded_files:
        file_path = get_file_path(uploaded_file)
        file_paths.append(file_path)
        file_data_list.append((file_path, uploaded_file.name))

if "response_history" not in st.session_state:
    st.session_state.response_history = []

if st.button("Process Audio"):
    if uploaded_files:
        try:
            with st.spinner('Processing audio...'):
                response = requests.post(
                    "http://localhost:8000/api/process_audio",
                    json={"file_data": file_data_list}
                )

                if response.status_code == 200:
                    st.success("Processing audio was successful!")

                    results = response.json()["result"]
                    for idx, uploaded_file in enumerate(uploaded_files):
                        with open(file_paths[idx], "rb") as f:
                            file_bytes = f.read()
                        st.session_state.response_history.append({
                            "file_name": uploaded_file.name,
                            "file_type": uploaded_file.type,
                            "original_audio": uploaded_file,
                            "cleaned_audio": results[idx]['cleaned_audio']
                        })
                else:
                    st.error(f"Error: {response.status_code}")

        except requests.exceptions.ConnectionError as e:
            st.error("Failed to connect to the server. Make sure the FastAPI server is running.")
    else:
        st.warning("Please upload a file before submitting.")

if st.session_state.response_history:
    st.subheader("Audio Processing History")
    for i, item in enumerate(st.session_state.response_history, 1):
        st.markdown(f"**Run {i}:**")
        st.markdown(f"**File Name:** {item['file_name']}")
        st.markdown(f"**File Type:** {item['file_type']}")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Original Audio:**")
            try:
                st.audio(item['original_audio'], format=item['file_type'])
            except Exception as e:
                st.error(f"Error playing original audio: {str(e)}")

        with col2:
            st.markdown("**Cleaned Audio:**")
            try:
                st.audio(item['cleaned_audio'], format='audio/mp3')
            except Exception as e:
                st.error(f"Error playing audio: {e}")

        st.markdown("---")
        

### File: ./defense_quickstart_denoise/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    engine = {
        'name': 'restack_engine',
        'image': 'ghcr.io/restackio/restack:main',
        'portMapping': [
            {
                'port': 5233,
                'path': '/',
                'name': 'engine-frontend',
            },
            {
                'port': 6233,
                'path': '/api',
                'name': 'engine-api',
            }
        ],
        'environmentVariables': [
          {
              'name': 'RESTACK_ENGINE_ID',
              'value': os.getenv('RESTACK_ENGINE_ID'),
          },
          {
              'name': 'RESTACK_ENGINE_ADDRESS',
              'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
          },
          {
              'name': 'RESTACK_ENGINE_API_KEY',
              'value': os.getenv('RESTACK_ENGINE_API_KEY'),
          },
        ],
    }

    # Define the application configuration
    app = {
        'name': 'defense_quickstart_denoise',
        'dockerFilePath': '/defense_quickstart_denoise/Dockerfile',
        'dockerBuildContext': './defense_quickstart_denoise/',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
            {
                'name': 'OPENBABYLON_API_URL',
                'value': os.getenv('OPENBABYLON_API_URL'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'development environment python',
        'previewEnabled': False,
        'applications': [engine,app],
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./defense_quickstart_denoise/src/services.py ###
import asyncio
from src.client import client
from src.workflows.child import ChildWorkflow
from src.workflows.parent import ParentWorkflow
from src.functions.denoise import denoise

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[ParentWorkflow, ChildWorkflow],
            functions=[denoise]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./defense_quickstart_denoise/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)

### File: ./defense_quickstart_denoise/src/workflows/parent.py ###
from restack_ai.workflow import workflow, log, workflow_info
from dataclasses import dataclass
from .child import ChildWorkflow

@dataclass
class WorkflowInputParams:
    file_data: list[tuple[str, str]]

@workflow.defn()
class ParentWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        parent_workflow_id = workflow_info().workflow_id

        log.info("ParentWorkflow started", input=input)

        child_workflow_results = []
        for file_data in input.file_data:
            result = await workflow.child_execute(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-execute-{file_data[0]}", input=WorkflowInputParams(file_data=file_data))
            child_workflow_results.append(result)

        log.info("ParentWorkflow completed", results=child_workflow_results)

        return child_workflow_results

### File: ./defense_quickstart_denoise/src/workflows/child.py ###
from datetime import timedelta
from dataclasses import dataclass
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.denoise import denoise, FunctionInputParams as DenoiseFunctionInputParams

@dataclass
class WorkflowInputParams:
    file_data: tuple[str, str]


@dataclass
class WorkflowOutputParams:
    cleaned_audio: str


@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams) -> WorkflowOutputParams:
        log.info("ChildWorkflow started", input=input)

        cleaned_audio = await workflow.step(
            denoise,
            DenoiseFunctionInputParams(file_data=input.file_data),
            start_to_close_timeout=timedelta(seconds=120)
        )

        log.info("ChildWorkflow completed", cleaned_audio=cleaned_audio)
        return WorkflowOutputParams(cleaned_audio=cleaned_audio)

### File: ./defense_quickstart_denoise/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dataclasses import dataclass
from src.client import client
import time
import uvicorn


@dataclass
class QueryRequest:
    file_data: list[tuple[str, str]]

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the Quickstart: War Denoise example"

@app.post("/api/process_audio")
async def schedule_workflow(request: QueryRequest):
    try:
        workflow_id = f"{int(time.time() * 1000)}-parent_workflow"
        
        runId = await client.schedule_workflow(
            workflow_name="ParentWorkflow",
            workflow_id=workflow_id,
            input={"file_data": request.file_data}
        )
        print("Scheduled workflow", runId)
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {
            "result": result,
            "workflow_id": workflow_id,
            "run_id": runId
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# Remove Flask-specific run code since FastAPI uses uvicorn
def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./defense_quickstart_denoise/src/functions/denoise.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
import sieve

@dataclass
class FunctionInputParams:
    file_data: tuple[str, str]

@function.defn()
async def denoise(input: FunctionInputParams):
    try:
        log.info("denoise function started", input=input)

        file_path, _ = input.file_data

        file = sieve.File(path=file_path)
        backend = "aicoustics"
        task = "all"
        enhancement_steps = 64

        audio_enhance = sieve.function.get("sieve/audio-enhance")
        output = audio_enhance.run(file, backend, task, enhancement_steps).path

        log.info("denoise function completed", output=output)
        return output        

    except Exception as e:
        log.error("denoise function failed", error=e)
        raise e


### File: ./stripe-ai/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dotenv import load_dotenv

load_dotenv()

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-CreatePaymentLinkWorkflow"

    run_id = await client.schedule_workflow(
        workflow_name="CreatePaymentLinkWorkflow",
        workflow_id=workflow_id,
    )

    result = await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    print(result)

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()


### File: ./stripe-ai/src/services.py ###
import asyncio
from src.client import client
from src.workflows.payment_link import CreatePaymentLinkWorkflow
from src.functions.create_payment_link import create_payment_link

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[CreatePaymentLinkWorkflow],
            functions=[create_payment_link]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./stripe-ai/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)


### File: ./stripe-ai/src/workflows/payment_link.py ###
from restack_ai.workflow import workflow, import_functions, log, RetryPolicy
from datetime import timedelta

with import_functions():
    from src.functions.create_payment_link import create_payment_link

@workflow.defn()
class CreatePaymentLinkWorkflow:
    @workflow.run
    async def run(self):
        log.info("CreatePaymentLinkWorkflow started", input=input)

        result = await workflow.step(
            create_payment_link,
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=10),
                backoff_coefficient=1,
            ),
        )

        return result


### File: ./stripe-ai/src/functions/create_payment_link.py ###
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain import hub
from stripe_agent_toolkit.langchain.toolkit import StripeAgentToolkit
from restack_ai.function import function, FunctionFailure
from langchain_openai import ChatOpenAI

import os
from dotenv import load_dotenv
from pydantic import SecretStr

load_dotenv()

@function.defn()
async def create_payment_link():
    stripe_secret_key = os.getenv("STRIPE_SECRET_KEY")
    openai_api_key = os.getenv("OPENAI_API_KEY")
    langchain_api_key = os.getenv("LANGCHAIN_API_KEY")

    if stripe_secret_key is None:
        raise FunctionFailure("STRIPE_SECRET_KEY is not set", non_retryable=True)
    
    if langchain_api_key is None:
       raise FunctionFailure("LANGCHAIN_API_KEY is not set", non_retryable=True)
    
    if openai_api_key is None:
        raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
    try:
        stripe_agent_toolkit = StripeAgentToolkit(
            secret_key=stripe_secret_key,
            configuration={
            "actions": {
                "payment_links": {
                    "create": True,
                },
                "products": {
                    "create": True,
                },
                "prices": {
                    "create": True,
                },
            }
            },
        )

        model = ChatOpenAI(api_key=SecretStr(openai_api_key))
        
        prompt = hub.pull("hwchase17/structured-chat-agent")

        agent = create_structured_chat_agent(model, stripe_agent_toolkit.get_tools(), prompt)
        agent_executor = AgentExecutor(agent=agent, tools=stripe_agent_toolkit.get_tools())

        result = agent_executor.invoke({
          "input": "Create a payment link for a new product called \"Test\" with a price of $100."
        })

        return result["output"]
    except Exception as e:
        raise FunctionFailure(f"Error creating payment link: {e}", non_retryable=True)


### File: ./gemini_generate/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass

@dataclass
class InputParams:
    user_content: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GeminiGenerateOppositeWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="GeminiGenerateOppositeWorkflow",
        workflow_id=workflow_id,
        input=InputParams(user_content="The opposite of hot is")
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./gemini_generate/src/services.py ###
import asyncio
from src.client import client
from src.functions.function import gemini_generate_opposite
from src.workflows.gemini_generate_content import GeminiGenerateOppositeWorkflow
async def main():
    await client.start_service(
        workflows= [GeminiGenerateOppositeWorkflow],
        functions= [gemini_generate_opposite]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./gemini_generate/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./gemini_generate/src/workflows/gemini_generate_content.py ###
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.function import gemini_generate_opposite, FunctionInputParams

@dataclass
class WorkflowInputParams:
    user_content: str

@workflow.defn()
class GeminiGenerateOppositeWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("GeminiGenerateOppositeWorkflow started", input=input)
        result = await workflow.step(
            gemini_generate_opposite,
            FunctionInputParams(user_content=input.user_content),
            start_to_close_timeout=timedelta(seconds=120)
        )
        log.info("GeminiGenerateOppositeWorkflow completed", result=result)
        return result


### File: ./gemini_generate/src/functions/function.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
import google.generativeai as genai

import os

@dataclass
class FunctionInputParams:
    user_content: str

@function.defn()
async def gemini_generate_opposite(input: FunctionInputParams) -> str:
    try:
        log.info("gemini_generate_opposite function started", input=input)
        genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-1.5-flash")

        response = model.generate_content(input.user_content)
        log.info("gemini_generate_opposite function completed", response=response.text)
        return response.text
    except Exception as e:
        log.error("gemini_generate_opposite function failed", error=e)
        raise e


### File: ./weaviate_search/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def schedule_workflow(workflow_name):

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-{workflow_name}"
    runId = await client.schedule_workflow(
        workflow_name=workflow_name,
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_seed_workflow():
    asyncio.run(schedule_workflow("SeedWorkflow"))

def run_schedule_search_workflow():
    asyncio.run(schedule_workflow("SearchWorkflow"))

if __name__ == "__main__":
    run_schedule_seed_workflow()


### File: ./weaviate_search/src/services.py ###
import asyncio
from src.functions.seed_database import seed_database
from src.functions.vector_search import vector_search
from src.client import restack_client
from src.workflows.workflow import SeedWorkflow, SearchWorkflow
from dotenv import load_dotenv
load_dotenv()

async def main():
    await restack_client.start_service(
        workflows= [SeedWorkflow, SearchWorkflow],
        functions= [seed_database, vector_search]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./weaviate_search/src/client.py ###
from restack_ai import Restack

restack_client = Restack()

### File: ./weaviate_search/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.seed_database import seed_database
    from src.functions.vector_search import vector_search

@workflow.defn()
class SeedWorkflow:
    @workflow.run
    async def run(self):
        seed_result = await workflow.step(seed_database, start_to_close_timeout=timedelta(seconds=120))
        log.info("SeedWorkflow completed", seed_result=seed_result)
        return seed_result

@workflow.defn()
class SearchWorkflow:
    @workflow.run
    async def run(self):
        search_result = await workflow.step(vector_search, start_to_close_timeout=timedelta(seconds=120))
        log.info("SearchWorkflow completed", search_result=search_result)
        return search_result


### File: ./weaviate_search/src/functions/weaviate_client.py ###
import os
import weaviate
from weaviate.classes.init import Auth

def get_weaviate_client():
		weaviate_url = os.environ["WEAVIATE_URL"]
		weaviate_api_key = os.environ["WEAVIATE_API_KEY"]

		client = weaviate.connect_to_weaviate_cloud(
				cluster_url=weaviate_url,
				auth_credentials=Auth.api_key(weaviate_api_key),
		)
		return client


### File: ./weaviate_search/src/functions/vector_search.py ###
from restack_ai.function import function, log
import weaviate.classes as wvc
from src.functions.weaviate_client import get_weaviate_client

@function.defn()
async def vector_search() -> str:
    client = get_weaviate_client()
    try:
        questions = client.collections.get("Question")

        query_vector = [0.0042927247,-0.007413445,0.00034457954,-0.01897398,-0.00251218,0.020693844,-0.027351381,-0.008647864,-0.000042449385,-0.012337249,-0.006678342,0.0072608762,0.0051838635,-0.020555146,0.00017055604,0.028821588,0.047351733,-0.0045319796,0.008959935,-0.012323379,-0.008585449,0.022483058,0.0013869869,-0.01801696,-0.0014806085,0.01269093,0.021539906,-0.03592296,-0.0024116235,-0.012087591,0.014369184,-0.0019261781,-0.03506303,-0.028932547,-0.008162417,-0.026949156,-0.011782453,0.00049628125,0.018044699,0.011213789,0.014452403,0.028072614,-0.010957196,-0.0006033393,-0.008876716,-0.008079199,-0.011789389,-0.007517469,-0.028960286,-0.0059328363,0.006758094,0.021997612,-0.004521577,-0.015506513,-0.010208224,-0.014577232,-0.004726158,0.025714736,-0.0025104464,-0.016421925,-0.025922785,-0.00038467214,-0.021054462,0.019611996,-0.013315074,0.0014346646,-0.0017857456,0.01572843,-0.01003485,-0.008675603,0.02323203,0.0033079637,0.0056970487,-0.0129405875,0.035534605,-0.010769953,-0.023010112,-0.001464138,-0.008932196,0.015700692,0.010166614,-0.022483058,0.00788502,0.0029698857,0.018710453,0.0029768206,-0.014341445,0.008897521,-0.013141701,-0.037115768,0.0148546295,0.004067339,-0.0060680676,0.02529864,-0.027601039,0.021345729,-0.010721409,0.033676043,-0.0191959,-0.06807332,-0.009812932,0.015312335,-0.010825433,-0.018613365,-0.02110994,0.006248376,0.008550774,0.0044556954,0.023134941,-0.026990766,-0.010097264,0.021623125,-0.0070979055,-0.04798975,0.0006449489,-0.0011338618,0.0033894493,-0.011172179,0.0053641717,-0.010430141,0.021484427,-0.012885109,0.010298378,-0.013003002,0.019917132,0.010832367,-0.04235858,-0.010222093,-0.0017658077,0.009466185,0.023564907,0.01998648,0.0036928526,0.01162295,-0.022413708,0.016421925,-0.008842042,0.0043065944,-0.017503774,-0.031262685,0.007739387,0.033787,-0.040583238,-0.00029863563,0.003266354,0.025423469,0.012579971,0.023759086,0.0168935,0.0042996593,0.015437164,-0.0035125443,0.029043505,0.0031709988,-0.0038350187,-0.0030479038,-0.009889216,0.008696408,0.0022053092,0.003027099,-0.009507795,0.0028918677,0.012406598,-0.022344358,0.009695038,0.017462164,0.007059763,0.012316444,-0.0020596755,-0.019487167,-0.0132734645,0.038197618,-0.022385968,0.030153096,0.00017857457,0.019736823,0.010624319,0.0026023341,-0.014715931,0.0079405,-0.024965765,0.026200183,0.018627234,0.0108670425,-0.0010913853,0.009181853,0.01484076,-0.0072192666,0.031207206,-0.020638365,-0.004466098,0.016061308,0.0054855333,0.004379411,-0.6910524,-0.032178096,0.009660364,-0.005471663,0.012371923,0.0264221,0.007586818,-0.0030929807,-0.012434337,0.011678429,0.019320728,-0.0058392147,-0.009098634,-0.015132027,-0.013571667,-0.030014396,0.008328856,-0.03281611,0.008481425,0.002597133,-0.024008743,-0.0026543462,-0.009015415,-0.011983567,0.009154114,0.0044799675,0.018155659,-0.018821413,-0.008890586,-0.007989044,-0.0075729485,0.0119211525,-0.000356499,-0.0021550308,0.019487167,0.0044314233,-0.0039737173,0.028280662,0.021317989,0.043828785,-0.01319718,-0.030097615,0.0011035214,0.012815759,-0.022691106,0.028211314,0.016602233,-0.0020648767,0.023981003,0.015756171,0.012337249,0.0019556514,0.01583939,0.018710453,0.02334299,-0.0115536,0.017531514,-0.008946066,-0.0024619016,-0.0015447567,-0.010069525,0.0008317587,-0.014743671,-0.027295902,-0.02328751,0.027878437,-0.032483235,0.009743583,0.019917132,-0.010215159,0.022330489,0.022815935,-0.014604972,-0.0008096536,-0.0001595035,0.016075179,0.022372099,-0.0052601476,-0.015132027,0.0032004723,0.0068309107,-0.010083395,-0.02106833,-0.0066228625,0.028904807,-0.020915762,-0.007628428,0.010478686,0.0037240598,0.0037136574,0.014299835,0.02868289,0.0009223463,-0.0052428106,-0.014702061,0.009701974,-0.011810194,-0.0084952945,0.00659859,-0.013044612,-0.005218538,0.000095517884,0.021248639,0.010346922,0.011352488,0.0045909267,-0.004670678,0.020763194,0.044022962,-0.021539906,0.0077809966,-0.00095181976,-0.0062934533,0.010714474,-0.00017369844,-0.033592824,0.0066089923,-0.0025104464,0.021997612,-0.01805857,-0.0023474754,0.0060819373,0.006276116,-0.011962762,0.0061720917,0.01590874,-0.018946242,-0.020888023,-0.017670212,0.0003892232,0.017795041,0.013377489,0.011269269,-0.015464904,0.028252924,0.009681169,0.015534253,-0.020541277,0.010679799,-0.007836476,-0.027073985,-0.015561993,0.005603427,-0.0010081661,-0.013342814,-0.006154754,-0.022427578,-0.0022053092,-0.019279119,0.012933653,0.017448295,0.012815759,0.0019105745,0.028849328,0.034674674,-0.023689736,-0.0062449086,-0.0008872382,-0.0068690525,0.013516188,0.010561905,0.01814179,-0.018391447,0.003644308,-0.0026716834,-0.017032199,-0.0070181536,0.015173636,-0.0076006884,-0.04776783,0.01110283,-0.03919625,-0.015451034,0.010242898,0.011123635,0.0003929074,-0.017808912,0.01603357,0.021387339,-0.0055479477,-0.016297096,-0.0038107466,-0.020333229,-0.0032958277,0.018405316,0.0031796675,0.032316796,0.020555146,-0.004365541,0.012933653,0.009951631,0.02227501,0.0012266166,0.0085299695,0.019182028,0.009819867,0.0014468007,0.010020981,-0.0030704422,0.02314881,0.04449454,-0.00095702097,0.01794761,-0.011095895,0.0017319999,-0.042968854,-0.0022139777,-0.038197618,0.021595387,0.015437164,0.0070909704,-0.020236138,0.0027913111,-0.039473645,0.012614646,0.012316444,-0.009743583,0.001855095,-0.019681344,-0.009743583,0.00084866263,-0.017157027,0.00058080076,-0.00075634127,0.0057143862,0.017198637,0.011858738,0.012420468,-0.00026157705,-0.013849064,0.017420555,-0.0037517995,-0.001249155,0.0019539178,0.00023535434,0.02323203,0.013439903,-0.018987851,0.039751045,0.018641103,-0.0019660539,0.009119439,0.01319718,-0.0072678113,0.010464816,-0.008821237,0.016505145,0.008710277,0.0009656896,-0.0037726043,-0.006210234,-0.0020388707,-0.033620562,-0.001653115,0.01678254,-0.021359598,0.016297096,0.022857545,0.012240159,0.026935285,0.009743583,0.005721321,0.004119351,-0.0048717917,0.008821237,0.0018377576,0.00633853,-0.01159521,-0.023398468,0.008412075,-0.03287159,-0.022496928,0.0030635074,-0.01271867,0.009188788,0.021539906,-0.0035749588,0.0062969206,-0.008751887,-0.0121846795,-0.00661246,-0.0108670425,0.00954247,0.017517645,0.0061408845,-0.013509252,-0.012191615,-0.010485621,-0.00028606606,0.009965501,-0.01697672,0.0140640475,0.012538361,0.0042476472,-0.0034223902,-0.010804628,0.007732452,-0.014494013,-0.010423207,-0.0030756434,-0.0034657335,-0.0132387895,0.0005292222,-0.006678342,0.0117269745,0.008800432,-0.0062449086,-0.022427578,-0.018419186,-0.017795041,0.012288704,0.0028329208,-0.0077948663,0.0013878538,-0.0072192666,0.014604972,-0.008328856,-0.005766398,0.026852066,-0.0041505583,-0.019209769,-0.005579155,-0.02644984,-0.013682626,0.074231535,0.022982374,-0.011747779,0.04737947,0.0021376936,-0.023315249,-0.010811563,-0.0041366885,0.00527055,-0.01907107,0.006158222,-0.027060114,0.018280488,0.005957109,-0.011858738,0.0072539416,-0.014452403,-0.02220566,0.015326206,-0.0036928526,-0.017906,-0.024008743,0.0070008165,0.027129464,0.0033287685,0.0022798597,-0.0025416536,0.02231662,-0.006650602,-0.026269533,-0.003377313,0.0046360036,0.008536904,0.012462078,0.0075660134,0.019223638,0.016740931,0.0013055014,0.0001641629,-0.012545297,0.01585326,0.020929633,0.010069525,-0.018779803,0.0101042,-0.021775695,-0.0098476065,0.033481862,-0.00319007,-0.0024237595,0.019501036,0.006761561,-0.0073787705,-0.016879631,0.009403771,0.009112504,0.0047608325,0.015367815,-0.007177657,-0.00002692705,-0.027129464,-0.044217143,0.020444186,0.0017944142,0.0011052552,-0.009653429,-0.038475018,0.0013497117,-0.023592647,0.000589036,0.0035957636,-0.029820219,-0.00023080329,-0.022496928,0.008079199,-0.00632466,0.029293163,-0.011928087,0.0046810806,0.030125355,-0.0018932371,0.0013809188,-0.0026526125,-0.030069876,0.0044418257,0.0068447804,-0.0009275475,-0.0065188385,0.011401032,0.021900523,0.0061339494,0.004902999,0.02653306,0.005773333,-0.0012517556,0.00581841,0.01994487,0.020263879,-0.00839127,-0.012323379,0.0025728608,-0.023065591,-0.00661246,0.010485621,0.008210963,0.0032351469,0.0011980099,0.031290423,-0.014161136,0.007725517,0.028849328,-0.007049361,0.018155659,0.0148685,0.0041332208,0.019528776,0.0056450367,0.01801696,0.005246278,-0.0220947,-0.001432064,-0.028100355,0.013689561,0.017642474,-0.0050139576,-0.016796412,0.00212209,-0.016560623,-0.013849064,0.0007424714,-0.016089048,0.023967134,0.0030739098,-0.032677412,-0.03206714,-0.009986306,-0.019833913,0.021886652,-0.020818673,-0.011498122,0.0005365906,0.005274018,0.021914393,-0.023093332,-0.0018446926,-0.047240775,-0.01058271,-0.0022538537,-0.014923979,0.024674498,-0.013960023,-0.0022746585,-0.007864215,-0.010769953,-0.0071637873,-0.049238034,-0.022011481,-0.019958742,0.043745566,0.018641103,0.019847782,-0.016435795,0.01697672,0.016158398,0.0028953352,0.0068690525,0.004972348,0.013793585,-0.035867482,0.0191959,0.028655149,-0.006137417,0.017725693,-0.008897521,0.0023145343,0.04208118,-0.003514278,-0.020901892,-0.0026006005,-0.0135369925,-0.014098722,0.0072400714,-0.0021966405,0.014979458,-0.009244268,-0.001380052,0.006449489,-0.0015967686,-0.012108396,-0.019931002,0.03292707,-0.024521928,0.031013027,-0.017545383,-0.004667211,-0.009473121,-0.0020683443,-0.008536904,-0.002578062,0.008113873,0.0028294532,0.019736823,0.00086599996,0.0027965123,-0.0048059095,0.0063940096,-0.0034900059,-0.011525861,0.010624319,-0.014618842,0.010506426,-0.0016297096,-0.016200008,-0.03284385,0.0064078793,0.011241529,-0.0075452086,0.01693511,-0.0121846795,-0.010284508,0.038585976,-0.0045077074,0.013966958,-0.012808824,0.030985286,0.037726045,0.0015369549,-0.026796587,-0.008370466,-0.0013289069,-0.0003181401,0.02323203,0.0074689244,0.007621493,-0.027878437,0.0070181536,0.009154114,-0.012559166,-0.03706029,0.021803435,0.016297096,0.0012985665,-0.019778432,0.00033179327,-0.024951894,0.026297271,0.0065708505,-0.003065241,-0.013100091,-0.008516099,0.0007281681,0.009313617,0.0016600499,0.03270515,0.011879543,0.005801073,-0.012635451,-0.0038904983,-0.013044612,0.009459251,-0.0046845484,0.024050353,-0.01572843,0.0044834353,0.005048632,-0.0076769725,0.0003972417,-0.012469012,-0.004670678,0.03481337,-0.018322097,-0.0064182817,-0.011616015,0.02657467,0.019917132,0.010624319,-0.0038731608,-0.017725693,-0.026227923,-0.0062449086,0.022594016,0.023925524,0.0011884744,-0.012087591,-0.0014702061,-0.0063697374,-0.007857281,-0.0042615174,-0.000051063875,-0.015561993,-0.007808736,0.009944696,0.00954247,0.017808912,0.0052948226,-0.015783912,-0.0026543462,0.024660626,-0.02337073,0.027448472,-0.02117929,-0.016213877,-0.025423469,0.0069141295,-0.0012768948,-0.025173813,0.02539573,-0.017059939,-0.023551038,-0.012420468,-0.020901892,0.018211138,0.0026803522,-0.02016679,0.019667475,-0.01114444,0.017323466,-0.033287685,-0.043662347,0.023010112,-0.014396924,0.0055375453,0.0048891287,-0.011026545,0.009805998,0.014285965,0.013183311,-0.006137417,-0.024840936,-0.017184768,-0.026075354,0.020846413,-0.00029235083,-0.022066962,-0.013786649,-0.011511991,-0.012656256,0.012045981,0.0026283402,-0.0051838635,-0.0026664822,-0.008259507,0.023981003,0.011858738,-0.021623125,-0.0034657335,-0.0028936013,-0.010707539,-0.023426209,0.0075244037,-0.00037773722,0.030319534,0.0071707224,-0.0052358755,-0.018322097,0.010138874,-0.024147442,-0.018710453,0.024410969,0.005291355,0.021872783,0.0001582032,0.0069279997,0.0029109388,-0.0015733633,0.015284596,-0.026685627,-0.0052636154,0.0025173812,-0.006130482,0.02231662,-0.009133309,-0.010978001,-0.023981003,0.021581516,-0.007063231,0.0048925965,0.00223825,-0.004847519,0.0050659697,-0.0082664415,0.023634257,0.010430141,0.014244355,0.015367815,-0.019501036,-0.009930826,0.013301204,-0.015340075,-0.024757717,-0.008065329,-0.016338706,-0.008751887,0.036366798,-0.011574405,-0.0129405875,0.022108572,-0.010326117,0.015783912,0.018821413,-0.0024358958,0.0066401996,0.0016513813,-0.007878086,-0.014369184,-0.0057109185,-0.027642649,-0.0047296253,0.014729801,-0.00065751845,0.020763194,-0.01210146,0.0009795595,-0.013349749,-0.0024653692,-0.024258401,-0.0013471111,-0.0051630586,0.015298465,0.021650866,-0.02314881,-0.02427227,0.0000668571,-0.018322097,-0.007933565,-0.011498122,0.010908652,-0.0074758595,0.0090570245,0.005100644,-0.018419186,-0.014383054,-0.0051457216,0.012788019,-0.022843674,-0.010138874,0.17021103,-0.031345904,0.0020614092,0.024091963,-0.004341269,0.005180396,0.019182028,-0.0011312612,0.0013644483,0.016408054,0.0028173171,-0.0062379735,-0.01106122,0.00850223,0.008897521,-0.0047747022,-0.02334299,-0.012801889,-0.0202084,-0.014181941,0.0033305022,-0.0015196175,-0.0083011165,-0.03167878,0.031123986,0.0016505144,-0.01003485,-0.021997612,0.004341269,-0.0058946945,-0.011241529,-0.016477404,-0.00028324872,-0.009785193,-0.019639734,0.0076561677,-0.0029525484,0.0037275273,0.016477404,0.00036668466,0.0036720478,0.010783823,0.029209943,-0.027573299,0.0009691571,0.034646932,-0.0055652848,-0.017906,0.009133309,0.01592261,-0.031651042,0.01803083,-0.0057906704,0.032400016,0.0072955512,-0.0072539416,0.013030742,0.0019747226,0.006775431,0.023564907,-0.027143333,0.01586713,-0.018585624,0.018114049,-0.01059658,0.010360792,-0.011310878,0.010125005,0.0028398556,-0.015312335,-0.015104287,-0.0025936656,-0.022066962,0.010395466,-0.028128095,-0.019889392,0.041581865,0.0356733,0.0190572,0.010055655,0.000041853415,0.0025919317,-0.018918501,-0.022372099,-0.010749148,-0.027198814,0.015284596,0.020263879,-0.014937849,0.01803083,-0.019556515,-0.0023925523,-0.020569015,-0.024008743,0.010201288,0.018086309,-0.006560448,0.030402754,-0.029154465,0.0106381895,-0.028627409,0.016768672,0.03181748,0.0007095305,-0.0006332462,0.0031467266,-0.027184943,0.0055340775,0.0022573213,-0.018710453,0.0006384474,-0.017378947,-0.005128384,-0.008398206,0.0022642561,0.019806173,0.0023682802,-0.016283225,0.0190572,-0.013190245,-0.015007198,-0.010229029,0.0122193545,0.011331683,-0.008516099,-0.007989044,-0.014188876,0.018238878,0.009043154,-0.040444538,-0.011213789,-0.0019383142,0.0014476676,-0.006449489,-0.017254118,-0.0006939269,-0.018835282,-0.013703431,-0.02434162,0.011747779,-0.0025364524,0.0007021621,-0.0036339057,0.007489729,-0.0034848046,-0.014951719,0.03167878,0.012753344,-0.005176929,-0.022732716,-0.016505145,-0.005097177,-0.026255662,-0.008842042,0.010887847,-0.000096547294,-0.00951473,-0.03481337,0.0070146862,0.007975175,-0.022427578,0.0053086923,0.03511851,-0.012302574,0.013335879,-0.018654974,-0.18252748,0.0028346544,0.024577407,-0.029709259,0.027864566,0.025797956,0.002408156,-0.0002698123,0.005759463,0.0030877795,0.012801889,0.0074550547,-0.04152639,-0.0018689649,-0.012857368,-0.0084952945,-0.000054964774,0.011331683,0.0029178737,0.010804628,0.023828436,-0.020693844,0.011498122,-0.022774326,0.007115243,0.018405316,0.0005461261,0.009327487,-0.0040846765,-0.028544191,-0.010686734,-0.018488536,0.017281856,-0.0012309508,-0.0150488075,0.014799151,-0.011872608,-0.027711999,-0.01708768,0.008321921,0.0059120315,0.040860634,0.013446838,-0.0037483322,0.005152656,0.012073721,0.009001545,0.012788019,0.027351381,-0.026366621,0.0042337775,-0.0018013492,0.0030756434,0.005430054,0.033315424,0.0012734274,-0.043939743,0.025950525,-0.02099898,-0.006151287,-0.021012852,-0.028433232,0.020721585,-0.016879631,-0.010520295,-0.0135369925,-0.024258401,0.0103746625,-0.019931002,0.003027099,0.008204027,-0.013176376,0.024716107,-0.012919783,0.0058808243,-0.0012612912,0.0066540698,0.010811563,-0.0015248187,-0.0140640475,-0.02228888,0.025978265,-0.012489817,-0.018807542,0.0085646445,-0.0014832091,-0.0016305764,0.0018308227,-0.007056296,-0.016019698,0.0046186666,-0.024091963,-0.018738193,-0.021470558,0.01482689,0.027947785,-0.007850346,0.012288704,-0.0026976895,-0.014563362,0.014175006,-0.00047027523,-0.0048856614,0.011505056,0.04760139,-0.0046498734,0.019667475,-0.008536904,0.027975526,0.0023006645,-0.009570209,-0.0010359058,0.012018242,0.035867482,-0.0077671264,0.007760192,-0.012510622,-0.01159521,0.0038731608,-0.009951631,0.040943854,-0.0036616453,-0.0190572,0.0127464095,0.0006341131,-0.015353945,-0.10768566,-0.02848871,-0.00011756881,0.02644984,-0.011935023,0.021942133,-0.031207206,0.014674322,0.0074273148,0.031872958,-0.019445557,-0.002134226,-0.01893237,-0.010097264,0.0011399299,0.007933565,-0.0043308665,-0.009882282,-0.0069349343,0.036810633,0.005305225,-0.0076492326,0.006054198,0.014563362,-0.011401032,0.011851803,-0.0024896415,0.03706029,0.008349661,-0.0019209769,0.002571127,-0.019778432,0.007760192,-0.032316796,-0.008356596,0.0006618528,-0.014618842,-0.014341445,0.016047439,-0.0033374373,0.02314881,0.007205397,0.021207029,-0.017739562,0.011435707,-0.013710366,0.0012274834,0.004719223,-0.0022642561,-0.026963025,-0.010360792,0.00084172765,-0.018474666,-0.00082742434,0.0093899015,-0.013107026,-0.0028727967,0.02002809,-0.009632624,-0.0014754073,0.010353858,0.0016383782,-0.0068933247,0.04435584,0.008363531,-0.015742302,-0.02321816,-0.026130833,0.027850697,-0.023509428,0.016019698,0.03256645,-0.025936656,0.011151374,-0.019306857,0.0129405875,-0.016671583,0.010548036,0.02013905,-0.022815935,-0.009126374,-0.019140419,0.016130658,-0.020804804,0.026644018,0.011588275,-0.005128384,-0.006664472,0.008086134,0.002212244,-0.0153816845,0.008294182,0.021415077,-0.009854542,-0.0032299457,-0.0010731812,-0.01482689,-0.027309772,0.00066098594,0.010450946,-0.012316444,-0.007871151,-0.072456196,0.0009197457,0.008204027,-0.005371107,0.017628603,0.0020267346,0.005381509,-0.0079405,0.009896152,0.02965378,-0.015298465,0.018322097,-0.0013601141,-0.02432775,-0.019140419,-0.028294533,0.035534605,0.02016679,0.020041961,0.015312335,0.022663366,-0.0050798394,0.037975702,0.0039355755,-0.023856174,-0.007822606,-0.0023162682,0.010215159,-0.013897609,-0.028031005,0.01055497,-0.011338618,-0.00012883807,0.026865937,0.0074550547,-0.017434426,0.014077917,0.00034067867,-0.00013642316,0.06025071,-0.02525703,-0.03620036,-0.0070909704,-0.009438446,0.01215694,0.01800309,-0.027004635,0.02009744,0.0064529567,-0.01316944,0.0061998316,0.013550862,-0.02119316,-0.034646932,-0.012045981,-0.010409337,0.019681344,0.0080029145,0.011935023,-0.0077879312,0.028155833,0.048433583,0.0026526125,0.00042476473,-0.014168072,-0.00633853,-0.009230398,-0.008162417,0.012371923,-0.030957548,-0.0200697,-0.00948699,0.0026335414,0.005031295,0.019098809,-0.008308051,-0.02319042,0.008959935,-0.0030635074,0.028627409,0.025548298,0.016269356,-0.016574493,0.026741108,0.023634257,0.024993503,-0.01678254,0.009029285,0.012045981,0.0071637873,-0.007961305,0.015756171,0.0061270148,-0.003606166,0.026810456,0.014660452,-0.005825345,-0.014230486,0.005142254,0.0134676425,-0.014660452,-0.007080568,0.0054508587,-0.020555146,-0.0020926164,-0.0039667827,-0.026158573,-0.042663716,-0.009570209,0.013980828,0.019681344,0.004802442,-0.010721409,0.006702614,-0.025492819,0.016089048,-0.006990414,-0.01798922,-0.018169528,0.037753783,0.021359598,0.011775519,0.023634257,0.010936392,0.00850223,0.01214307,0.028877066,-0.0057698656,0.0021671671,0.0044140858,0.012801889,-0.008807367,-0.029293163,-0.010229029,-0.0018134854,-0.023842305,0.0101458095,0.017739562,-0.029431863,0.047934268,0.0026266065,-0.004015327,0.016089048,-0.02205309,0.012337249,0.014244355,0.020360967,-0.04260824,-0.010797693,0.009070895,-0.0006332462,0.0036789828,-0.0140293725,-0.032122616,-0.0036893853,-0.01904333,0.019265248,0.002271191,-0.0055132727,-0.0018464263,-0.0055028703,0.029764738,0.009327487,-0.02868289,-0.0085646445,0.014168072,-0.0011243263,-0.0057109185,-0.024022613,0.028932547,0.0028450568,-0.04629762,-0.011338618,0.0070250886,-0.00013393092,-0.021720216,-0.007961305,0.012801889,0.028072614,-0.0042407126,0.018682713,-0.019722953,-0.022538537,-0.0032507505,0.010645124,-0.021803435,-0.020763194,-0.025992135]

        response = questions.query.near_vector(
            near_vector=query_vector,
            limit=2,
            return_metadata=wvc.query.MetadataQuery(certainty=True)
        )

        log.info("vector_search function completed", response=response)

        return '\n\n Found nearest questions: \n\n' + str(response)
    finally:
        client.close()


### File: ./weaviate_search/src/functions/seed_database.py ###
import json
import requests
from restack_ai.function import function, log
import weaviate.classes as wvc
from src.functions.weaviate_client import get_weaviate_client


@function.defn()
async def seed_database() -> str:
    client = get_weaviate_client()

    try:
        # Create the collection. Weaviate's autoschema feature will infer properties when importing.
        if not client.collections.exists("Question"):
            questions = client.collections.create(
                "Question",
                vectorizer_config=wvc.config.Configure.Vectorizer.none(),
            )
        else:
            questions = client.collections.get("Question")

        fname = "jeopardy_tiny_with_vectors_all-OpenAI-ada-002.json"  # This file includes pre-generated vectors
        url = f"https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/{fname}"
        resp = requests.get(url)
        data = json.loads(resp.text)  # Load data

        question_objs = list()
        for i, d in enumerate(data):
            question_objs.append(wvc.data.DataObject(
                properties={
                    "answer": d["Answer"],
                    "question": d["Question"],
                    "category": d["Category"],
                },
                vector=d["vector"]
            ))

        questions = client.collections.get("Question")
        questions.data.insert_many(question_objs)    # This uses batching under the hood

        log.info("seed_database function completed")

        return "Data has been seeded"
    finally:
        client.close()


### File: ./streamlit_fastapi_togetherai_llama/frontend.py ###
import streamlit as st
import requests

# Set page title and header
st.title("Restack AI with TogetherAi + LlamaIndex")

# Create text area for user input with session state
if "user_input" not in st.session_state:
    st.session_state.user_input = ""

user_prompt = st.text_area("Ask the pirate any question", height=150, key="user_input")

# Initialize response history in session state
if "response_history" not in st.session_state:
    st.session_state.response_history = []

# Create button to send request
if st.button("Generate Response"):
    if user_prompt:
        try:
            # Make POST request to FastAPI backend
            response = requests.post(
                "http://localhost:8000/api/schedule",
                json={"prompt": user_prompt}
            )
            
            if response.status_code == 200:
                st.success("Response received!")
                # Add the new response to history with the original prompt
                st.session_state.response_history.append({
                    "prompt": user_prompt,
                    "response": response.json()["result"]
                })
            else:
                st.error(f"Error: {response.status_code}")
                
        except requests.exceptions.ConnectionError:
            st.error("Failed to connect to the server. Make sure the FastAPI server is running.")
    else:
        st.warning("Please enter a prompt before submitting.")

# Display response history
if st.session_state.response_history:
    st.subheader("Response History")
    for i, item in enumerate(st.session_state.response_history, 1):
        st.markdown(f"**Prompt {i}:** {item['prompt']}")
        st.markdown(f"**Response {i}:** {item['response']}")
        st.markdown("---")


### File: ./streamlit_fastapi_togetherai_llama/src/services.py ###
import asyncio
from src.client import client
from src.functions.function import llm_complete
from src.workflows.workflow import LlmCompleteWorkflow

async def main():
    await client.start_service(
        workflows= [LlmCompleteWorkflow],
        functions= [llm_complete]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./streamlit_fastapi_togetherai_llama/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./streamlit_fastapi_togetherai_llama/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.function import llm_complete, FunctionInputParams

@workflow.defn()
class LlmCompleteWorkflow:
    @workflow.run
    async def run(self, input: dict):
        log.info("LlmCompleteWorkflow started", input=input)
        prompt = input["prompt"]
        result = await workflow.step(llm_complete, FunctionInputParams(prompt=prompt), start_to_close_timeout=timedelta(seconds=120))
        log.info("LlmCompleteWorkflow completed", result=result)
        return result


### File: ./streamlit_fastapi_togetherai_llama/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dataclasses import dataclass
import time
from restack_ai import Restack
import uvicorn

# Define request model
@dataclass
class PromptRequest:
    prompt: str

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the TogetherAI LlamaIndex FastAPI App!"

@app.post("/api/schedule")
async def schedule_workflow(request: PromptRequest):
    try:
        client = Restack()
        workflow_id = f"{int(time.time() * 1000)}-LlmCompleteWorkflow"
        
        runId = await client.schedule_workflow(
            workflow_name="LlmCompleteWorkflow",
            workflow_id=workflow_id,
            input={"prompt": request.prompt}
        )
        print("Scheduled workflow", runId)
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# Remove Flask-specific run code since FastAPI uses uvicorn
def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./streamlit_fastapi_togetherai_llama/src/functions/function.py ###
from llama_index.llms.together import TogetherLLM
from restack_ai.function import function, log, FunctionFailure, log
from llama_index.core.llms import ChatMessage, MessageRole
import os
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class FunctionInputParams:
    prompt: str

@function.defn()
async def llm_complete(input: FunctionInputParams):
    try:
        log.info("llm_complete function started", input=input)
        api_key = os.getenv("TOGETHER_API_KEY")
        if not api_key:
            log.error("TOGETHER_API_KEY environment variable is not set.")
            raise ValueError("TOGETHER_API_KEY environment variable is required.")
    
        llm = TogetherLLM(
            model="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo", api_key=api_key
        )
        messages = [
            ChatMessage(
                # This is a system prompt that is used to set the behavior of the LLM. You can update this llm_complete function to also accept a system prompt as an input parameter.
                role=MessageRole.SYSTEM, content="You are a pirate with a colorful personality"
            ),
            ChatMessage(role=MessageRole.USER, content=input.prompt),
        ]
        resp = llm.chat(messages)
        log.info("llm_complete function completed", response=resp.message.content)
        return resp.message.content
    except Exception as e:
        log.error("llm_complete function failed", error=e)
        raise e
  

### File: ./human_loop/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-HumanLoopWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="HumanLoopWorkflow",
        workflow_id=workflow_id
    )

    await client.send_workflow_event(
        event_name="event_feedback",
        event_input={
            "feedback": "This is a human feedback"
        },
        workflow_id=workflow_id,
        run_id=runId,
    )

    end = await client.send_workflow_event(
        event_name="event_end",
        event_input={
            "end": True
        },
        workflow_id=workflow_id,
        run_id=runId,
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./human_loop/src/services.py ###
import asyncio
from src.functions.function import feedback, goodbye
from src.client import client
from src.workflows.workflow import HumanLoopWorkflow

async def main():

    await client.start_service(
        workflows= [HumanLoopWorkflow],
        functions= [feedback, goodbye]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./human_loop/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./human_loop/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass

with import_functions():
    from src.functions.function import feedback as feedback_function, goodbye, InputFeedback

@dataclass
class Feedback:
    feedback: str

@dataclass
class End:
    end: bool

@workflow.defn()
class HumanLoopWorkflow:
    def __init__(self) -> None:
        self.end_workflow = False
        self.feedbacks = []
    @workflow.event
    async def event_feedback(self, feedback: Feedback) -> Feedback:
        result = await workflow.step(feedback_function, InputFeedback(feedback.feedback), start_to_close_timeout=timedelta(seconds=120))
        log.info("Received feedback", result=result)
        return result
    
    @workflow.event
    async def event_end(self, end: End) -> End:
        log.info("Received end", end=end)
        self.end_workflow = end.end
        return end

    @workflow.run
    async def run(self):
        await workflow.condition(
            lambda: self.end_workflow
        )
        result = await workflow.step(goodbye, start_to_close_timeout=timedelta(seconds=120))
        log.info("Workflow ended", result=result)
        return result




### File: ./human_loop/src/functions/function.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
@dataclass
class InputFeedback:
    feedback: str

@function.defn()
async def goodbye() -> str:
    log.info("goodbye function started")
    return f"Goodbye!"

@function.defn()
async def feedback(input: InputFeedback) -> str:
    log.info("feedback function started", input=input)
    return f"Received feedback: {input.feedback}"


### File: ./flask_gemini/src/services.py ###
import asyncio
from src.client import client
from src.functions.function import gemini_generate
from src.workflows.gemini_generate_content import GeminiGenerateWorkflow
async def main():
    await client.start_service(
        workflows= [GeminiGenerateWorkflow],
        functions= [gemini_generate]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./flask_gemini/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./flask_gemini/src/workflows/gemini_generate_content.py ###
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.function import gemini_generate, FunctionInputParams

@dataclass
class WorkflowInputParams:
    user_content: str

@workflow.defn()
class GeminiGenerateWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("GeminiGenerateWorkflow started", input=input)
        result = await workflow.step(gemini_generate, FunctionInputParams(user_content=input.user_content), start_to_close_timeout=timedelta(seconds=120))
        log.info("GeminiGenerateWorkflow completed", result=result)
        return result


### File: ./flask_gemini/src/app.py ###
import time
from flask import Flask, jsonify, request
from dataclasses import dataclass
from restack_ai import Restack
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# Example route for the home page
@app.route('/')
def home():
    return "Welcome to the Flask App!"

@app.route('/test', methods=['GET', 'POST'])
def test_route():
    return 'This is a test route', 200

@dataclass
class InputParams:
    user_content: str
    
# New endpoint to schedule workflow and get back result
@app.route('/api/schedule', methods=['POST'])
async def schedule_workflow():
    if request.is_json:
        data = request.get_json()

        user_content = data.get('user_content', 'this is a story')
        
        client = Restack()

        workflow_id = f"{int(time.time() * 1000)}-GeminiGenerateWorkflow"
        runId = await client.schedule_workflow(
            workflow_name="GeminiGenerateWorkflow",
            workflow_id=workflow_id,
            input=InputParams(user_content=user_content)
        )

        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        return jsonify(result), 200

    else:
        return jsonify({'error': 'Request must be JSON'}), 400


def run_flask():
    app.run(debug=True) 

if __name__ == '__main__':
    run_flask()

### File: ./flask_gemini/src/functions/function.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
import google.generativeai as genai
import os

@dataclass
class FunctionInputParams:
    user_content: str

@function.defn()
async def gemini_generate(input: FunctionInputParams) -> str:
    try:
        log.info("gemini_generate function started", input=input)
        genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-1.5-flash")

        response = model.generate_content(input.user_content)
        log.info("gemini_generate function completed", response=response.text)
        return response.text
    except Exception as e:
        log.error("gemini_generate function failed", error=e)
        raise e


### File: ./get_started/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./get_started/schedule_calendar.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleCalendarSpec, ScheduleRange

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id,
        schedule=ScheduleSpec(
            calendars=[ScheduleCalendarSpec(
                day_of_week=[ScheduleRange(start=1)],
                hour=[ScheduleRange(start=9)]
            )]
        )
    )

    exit(0)

def run_schedule_calendar():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_calendar()

### File: ./get_started/schedule_interval.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleIntervalSpec
from datetime import timedelta

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id,
        schedule=ScheduleSpec(
            intervals=[ScheduleIntervalSpec(
                every=timedelta(minutes=10)
            )]
        )
    )

    exit(0)

def run_schedule_interval():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_interval()

### File: ./get_started/src/services.py ###
import asyncio
import os
from src.functions.function import welcome
from src.client import client
from src.workflows.workflow import GreetingWorkflow
from watchfiles import run_process
async def main():

    await client.start_service(
        workflows=[GreetingWorkflow],
        functions=[welcome]
    )

def run_services():
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Service interrupted by user. Exiting gracefully.")

def watch_services():
    watch_path = os.getcwd()
    print(f"Watching {watch_path} and its subdirectories for changes...")
    run_process(watch_path, recursive=True, target=run_services)



### File: ./get_started/src/client.py ###
import os
from restack_ai import Restack

client = Restack()


### File: ./get_started/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome

@workflow.defn()
class GreetingWorkflow:
    @workflow.run
    async def run(self):
        log.info("GreetingWorkflow started")
        result = await workflow.step(welcome, input="world", start_to_close_timeout=timedelta(seconds=120))
        log.info("GreetingWorkflow completed", result=result)
        return result




### File: ./get_started/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn()
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: ./defense_quickstart_audio_transcription_translation/frontend.py ###
import streamlit as st
import requests
import base64
# Set page title and header
st.title("Defense Hackathon Quickstart: War Audio Transcription & Translation")



uploaded_files = st.file_uploader("Choose a files", accept_multiple_files=True)

if uploaded_files:
    file_data_list = []
    for uploaded_file in uploaded_files:
        audio_data = uploaded_file.read()
        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        file_data_list.append((uploaded_file.name, audio_base64))

if "response_history" not in st.session_state:
    st.session_state.response_history = []

if st.button("Process Audio"):
    if uploaded_file:
        try:
            with st.spinner('Processing audio...'):
                response = requests.post(
                    "http://localhost:8000/api/process_audio",
                    json={"file_data": file_data_list}
                )

                if response.status_code == 200:
                    st.success("Processing audio was successful!")

                    results = response.json()["result"]
                    for idx, uploaded_file in enumerate(uploaded_files):
                        st.session_state.response_history.append({
                            "file_name": uploaded_file.name,
                            "file_type": uploaded_file.type,
                            "transcription": results[idx]['transcription'],
                            "translation": results[idx]['translation']
                    })
                else:
                    st.error(f"Error: {response.status_code}")

        except requests.exceptions.ConnectionError as e:
            st.error(f"Failed to connect to the server. Make sure the FastAPI server is running.")
    else:
        st.warning("Please upload a file before submitting.")

if st.session_state.response_history:
    st.subheader("Audio Processing History")
    for i, item in enumerate(st.session_state.response_history, 1):
        st.markdown(f"**Run {i}:**")
        st.markdown(f"**File Name:** {item['file_name']}")
        st.markdown(f"**File Type:** {item['file_type']}")
        st.markdown(f"**Transcription:** {item['transcription']}")
        st.markdown(f"**Translation:** {item['translation']}")
        st.markdown("---")
        

### File: ./defense_quickstart_audio_transcription_translation/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    engine = {
        'name': 'restack_engine',
        'image': 'ghcr.io/restackio/restack:main',
        'portMapping': [
            {
                'port': 5233,
                'path': '/',
                'name': 'engine-frontend',
            },
            {
                'port': 6233,
                'path': '/api',
                'name': 'engine-api',
            }
        ],
        'environmentVariables': [
          {
              'name': 'RESTACK_ENGINE_ID',
              'value': os.getenv('RESTACK_ENGINE_ID'),
          },
          {
              'name': 'RESTACK_ENGINE_ADDRESS',
              'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
          },
          {
              'name': 'RESTACK_ENGINE_API_KEY',
              'value': os.getenv('RESTACK_ENGINE_API_KEY'),
          },
        ],
    }

    # Define the application configuration
    app = {
        'name': 'defense_quickstart_audio_transcription_translation',
        'dockerFilePath': '/defense_quickstart_audio_transcription_translation/Dockerfile',
        'dockerBuildContext': './defense_quickstart_audio_transcription_translation/',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
            {
                'name': 'OPENBABYLON_API_URL',
                'value': os.getenv('OPENBABYLON_API_URL'),
            },
            {
                'name': 'GROQ_API_KEY',
                'value': os.getenv('GROQ_API_KEY'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'development environment python',
        'previewEnabled': False,
        'applications': [engine,app],
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./defense_quickstart_audio_transcription_translation/src/services.py ###
import asyncio
from src.client import client
from src.functions.transcribe import transcribe
from src.functions.translate import translate
from src.workflows.child import ChildWorkflow
from src.workflows.parent import ParentWorkflow

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[ParentWorkflow, ChildWorkflow],
            functions=[transcribe, translate]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./defense_quickstart_audio_transcription_translation/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)

### File: ./defense_quickstart_audio_transcription_translation/src/workflows/parent.py ###
from restack_ai.workflow import workflow, log, workflow_info
from dataclasses import dataclass
from .child import ChildWorkflow

@dataclass
class WorkflowInputParams:
    file_data: list[tuple[str, str]]

@workflow.defn()
class ParentWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        parent_workflow_id = workflow_info().workflow_id

        log.info("ParentWorkflow started", input=input)

        child_workflow_results = []
        for file_data in input.file_data:
            result = await workflow.child_execute(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-execute-{file_data[0]}", input=WorkflowInputParams(file_data=file_data))
            child_workflow_results.append(result)

        log.info("ParentWorkflow completed", results=child_workflow_results)

        return child_workflow_results




### File: ./defense_quickstart_audio_transcription_translation/src/workflows/child.py ###
from datetime import timedelta
from dataclasses import dataclass
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.transcribe import transcribe, FunctionInputParams as TranscribeFunctionInputParams
    from src.functions.translate import translate, FunctionInputParams as TranslationFunctionInputParams
    
@dataclass
class WorkflowInputParams:
    file_data: tuple[str, str]

@dataclass
class WorkflowOutputParams:
    transcription: str
    translation: str

@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("ChildWorkflow started", input=input)

        transcription = await workflow.step(
            transcribe,
            TranscribeFunctionInputParams(file_data=input.file_data),
            start_to_close_timeout=timedelta(seconds=120)
        )

        translation_prompt = f"""
        Instructions: Translate the following content to English. Output only the translated content.
        Content: {transcription['text']}
        """

        translation = await workflow.step(
            translate,
            TranslationFunctionInputParams(user_prompt=translation_prompt),
            start_to_close_timeout=timedelta(seconds=120)
        )

        log.info("ChildWorkflow completed", transcription=transcription['text'], translation=translation['content'])
        return WorkflowOutputParams(
            transcription=transcription['text'],
            translation=translation['content']
        )


### File: ./defense_quickstart_audio_transcription_translation/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dataclasses import dataclass
from src.client import client
import time
import uvicorn


@dataclass
class QueryRequest:
    file_data: list[tuple[str, str]]

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the Quickstart: War Audio Transcription & Translation example!"

@app.post("/api/process_audio")
async def schedule_workflow(request: QueryRequest):
    try:
        workflow_id = f"{int(time.time() * 1000)}-parent_workflow"
        
        runId = await client.schedule_workflow(
            workflow_name="ParentWorkflow",
            workflow_id=workflow_id,
            input={"file_data": request.file_data}
        )
        print("Scheduled workflow", runId)
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {
            "result": result,
            "workflow_id": workflow_id,
            "run_id": runId
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# Remove Flask-specific run code since FastAPI uses uvicorn
def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./defense_quickstart_audio_transcription_translation/src/functions/translate.py ###
from restack_ai.function import function, log
from openai import OpenAI
from dataclasses import dataclass
import os

@dataclass
class FunctionInputParams:
    user_prompt: str

@function.defn()
async def translate(input: FunctionInputParams):
    try:
        log.info("translate function started", input=input)
        if not os.environ.get("OPENBABYLON_API_URL"):
            raise Exception("OPENBABYLON_API_URL is not set")
        

        client = OpenAI(api_key='openbabylon',base_url=os.environ.get("OPENBABYLON_API_URL"))

        messages = []
        if input.user_prompt:
            messages.append({"role": "user", "content": input.user_prompt})
        print(messages)
        response = client.chat.completions.create(
            model="orpo-mistral-v0.3-ua-tokV2-focus-10B-low-lr-1epoch-aux-merged-1ep",
            messages=messages,
            temperature=0.0
        )
        log.info("translate function completed", response=response)
        return response.choices[0].message
    except Exception as e:
        log.error("translate function failed", error=e)
        raise e


### File: ./defense_quickstart_audio_transcription_translation/src/functions/transcribe.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
from groq import Groq
import os
import base64
@dataclass
class FunctionInputParams:
    file_data: tuple[str, str]

@function.defn()
async def transcribe(input: FunctionInputParams):
    try:
        log.info("transcribe function started", input=input)
        if not os.environ.get("GROQ_API_KEY"):
            raise Exception("GROQ_API_KEY is not set")
        
        client = Groq(api_key=os.environ.get("GROQ_API_KEY"))


        filename, base64_content = input.file_data
        file_bytes = base64.b64decode(base64_content)
        transcription = client.audio.transcriptions.create(
            file=(filename, file_bytes), # Required audio file
            model="whisper-large-v3-turbo", # Required model to use for transcription
            # Best practice is to write the prompt in the language of the audio, use translate.google.com if needed
            prompt=f"     ",  # Translation: Describe what the audio is about
            language="ru", # Original language of the audio
            response_format="json",  # Optional
            temperature=0.0  # Optional
        )

        log.info("transcribe function completed", transcription=transcription)
        return transcription
        

    except Exception as e:
        log.error("transcribe function failed", error=e)
        raise e


### File: ./production_demo/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ExampleWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="ChildWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./production_demo/schedule_scale.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ExampleWorkflow"
    await client.schedule_workflow(
        workflow_name="ExampleWorkflow",
        workflow_id=workflow_id,
    )

    exit(0)

def run_schedule_scale():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_scale()

### File: ./production_demo/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    engine = {
        'name': 'restack_engine',
        'image': 'ghcr.io/restackio/restack:main',
        'portMapping': [
            {
                'port': 5233,
                'path': '/',
                'name': 'engine-frontend',
            },
            {
                'port': 6233,
                'path': '/api',
                'name': 'engine-api',
            }
        ],
        'environmentVariables': [
          {
              'name': 'RESTACK_ENGINE_ID',
              'value': os.getenv('RESTACK_ENGINE_ID'),
          },
          {
              'name': 'RESTACK_ENGINE_ADDRESS',
              'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
          },
          {
              'name': 'RESTACK_ENGINE_API_KEY',
              'value': os.getenv('RESTACK_ENGINE_API_KEY'),
          },
        ],
    }

    # Define the application configuration
    backend = {
        'name': 'backend',
        'dockerFilePath': '/production_demo/Dockerfile',
        'dockerBuildContext': './production_demo/',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'production_demo',
        'previewEnabled': False,
        'applications': [engine,backend],
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./production_demo/schedule_interval.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleIntervalSpec
from datetime import timedelta

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ChildWorkflow"
    await client.schedule_workflow(
        workflow_name="ChildWorkflow",
        workflow_id=workflow_id,
        schedule=ScheduleSpec(
            intervals=[ScheduleIntervalSpec(
                every=timedelta(seconds=1)
            )]
        )
    )

    exit(0)

def run_schedule_scale():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_scale()

### File: ./production_demo/src/services.py ###
import asyncio
import os
from watchfiles import run_process

from src.client import client
from restack_ai.restack import ServiceOptions

from src.functions.function import example_function
from src.functions.generate import llm_generate
from src.functions.evaluate import llm_evaluate

from src.workflows.workflow import ExampleWorkflow, ChildWorkflow



async def main():

    await asyncio.gather(
        client.start_service(
            workflows=[ExampleWorkflow, ChildWorkflow],
            functions=[example_function],
            options=ServiceOptions(
                max_concurrent_workflow_runs=1000
            )

        ),
        client.start_service(
            task_queue="llm",
            functions=[llm_generate, llm_evaluate],
            options=ServiceOptions(
                rate_limit=1,
                max_concurrent_function_runs=1
            )
        )
    )

def run_services():
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Service interrupted by user. Exiting gracefully.")

def watch_services():
    watch_path = os.getcwd()
    print(f"Watching {watch_path} and its subdirectories for changes...")
    run_process(watch_path, recursive=True, target=run_services)



### File: ./production_demo/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)

### File: ./production_demo/src/workflows/child.py ###
from datetime import timedelta

from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.function import example_function
    from src.functions.generate import llm_generate
    from src.functions.evaluate import llm_evaluate

@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self):
        log.info("ChildWorkflow started")
        await workflow.step(example_function, input="first", start_to_close_timeout=timedelta(minutes=2))

        await workflow.sleep(1)

        generated_text = await workflow.step(
            llm_generate,
            "Generate a random joke in max 20 words.",
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=2)
        )

        evaluation = await workflow.step(
            llm_evaluate,
            generated_text,
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=2)
        )

        return {
            "generated_text": generated_text,
            "evaluation": evaluation
        }




### File: ./production_demo/src/workflows/workflow.py ###
import asyncio
from datetime import timedelta

from restack_ai.workflow import workflow, log, workflow_info, import_functions
from .child import ChildWorkflow

with import_functions():
    from src.functions.generate import llm_generate

@workflow.defn()
class ExampleWorkflow:
    @workflow.run
    async def run(self):
        # use the parent run id to create child workflow ids
        parent_workflow_id = workflow_info().workflow_id

        tasks = []
        for i in range(50):
            log.info(f"Queue ChildWorkflow {i+1} for execution")
            task = workflow.child_execute(
                ChildWorkflow, 
                workflow_id=f"{parent_workflow_id}-child-execute-{i+1}"
            )
            tasks.append(task)

        # Run all child workflows in parallel and wait for their results
        results = await asyncio.gather(*tasks)

        for i, result in enumerate(results, start=1):
            log.info(f"ChildWorkflow {i} completed", result=result)

        generated_text = await workflow.step(
            llm_generate,
            f"Give me the top 3 unique jokes according to the results. {results}",
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=2)
        )

        return {
            "top_jokes": generated_text,
            "results": results
        }



### File: ./production_demo/src/functions/generate.py ###
from restack_ai.function import function, FunctionFailure, log
from openai import OpenAI

@function.defn()
async def llm_generate(prompt: str) -> str:

    try:
        client = OpenAI(base_url="http://192.168.1.17:1234/v1/",api_key="llmstudio")
    except Exception as e:
        log.error(f"Failed to create LLM client {e}")
        raise FunctionFailure(f"Failed to create OpenAI client {e}", non_retryable=True) from e

    try:
        response = client.chat.completions.create(
            model="llama-3.2-3b-instruct",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.5,
        )
    
    except Exception as e:
        log.error(f"Failed to generate {e}")
    
    return response.choices[0].message.content



### File: ./production_demo/src/functions/evaluate.py ###
from restack_ai.function import function, FunctionFailure, log
from openai import OpenAI

@function.defn()
async def llm_evaluate(generated_text: str) -> str:
    try:
        client = OpenAI(base_url="http://192.168.1.17:1234/v1/",api_key="llmstudio")
    except Exception as e:
        log.error(f"Failed to create LLM client {e}")
        raise FunctionFailure(f"Failed to create OpenAI client {e}", non_retryable=True) from e

    prompt = (
        f"Evaluate the following joke for humor, creativity, and originality. "
        f"Provide a score out of 10 for each category for your score.\n\n"
        f"Joke: {generated_text}\n\n"
        f"Response format:\n"
        f"Humor: [score]/10"
        f"Creativity: [score]/10"
        f"Originality: [score]/10"
        f"Average score: [score]/10"
        f"Only answer with the scores"
    )

    try:
        response = client.chat.completions.create(
            model="llama-3.2-3b-instruct",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.5,
        )
    
    except Exception as e:
        log.error(f"Failed to generate {e}")
    
    return response.choices[0].message.content

### File: ./production_demo/src/functions/function.py ###
from restack_ai.function import function, log, FunctionFailure

tries = 0

@function.defn()
async def example_function(input: str) -> str:
    try:
        global tries

        if tries == 0:
            tries += 1
            raise FunctionFailure(message="Simulated failure", non_retryable=False)
      
        log.info("example function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("example function failed", error=e)
        raise e


### File: ./llama_quickstart/frontend.py ###
import streamlit as st
import requests

# Set page title and header
st.title("LLama Hackathon Quickstart")
st.text("FastAPI, Restack, Together AI, LLamaIndex")

# Create text area for user input with session state
if "user_input" not in st.session_state:
    st.session_state.user_input = ""

query = st.text_input("Query HN", key="query", value="ai")
count = st.number_input("Number of results", key="count", value=5)

# Initialize response history in session state
if "response_history" not in st.session_state:
    st.session_state.response_history = []

# Create button to send request
if st.button("Search HN"):
    if query:
        try:
            with st.spinner('Searching...'):
                # Make POST request to FastAPI backend
                response = requests.post(
                    "http://localhost:8000/api/schedule",
                    json={"query": query, "count": count}
                )
                
                if response.status_code == 200:
                    st.success("Response received!")
                    # Add the new response to history with the original prompt
                    st.session_state.response_history.append({
                        "query": query,
                        "count": count,
                        "response": response.json()["result"]
                    })
                else:
                    st.error(f"Error: {response.status_code}")
                    
        except requests.exceptions.ConnectionError:
            st.error("Failed to connect to the server. Make sure the FastAPI server is running.")
    else:
        st.warning("Please enter a prompt before submitting.")

# Display response history
if st.session_state.response_history:
    st.subheader("Response History")
    for i, item in enumerate(st.session_state.response_history, 1):
        st.markdown(f"**Query {i}:** {item['query']}")
        st.markdown(f"**Response {i}:** {item['response']}")
        st.markdown("---")


### File: ./llama_quickstart/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    engine = {
        'name': 'restack_engine',
        'image': 'restack/restack:main',
        'environmentVariables': [
          {
              'name': 'RESTACK_ENGINE_ID',
              'value': os.getenv('RESTACK_ENGINE_ID'),
          },
          {
              'name': 'RESTACK_ENGINE_ADDRESS',
              'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
          },
          {
              'name': 'RESTACK_ENGINE_API_KEY',
              'value': os.getenv('RESTACK_ENGINE_API_KEY'),
          },
        ],
    }

    # Define the application configuration
    app = {
        'name': 'llama_quickstart',
        'dockerFilePath': '/llama_quickstart/Dockerfile',
        'dockerBuildContext': './llama_quickstart/',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
            {
                'name': 'TOGETHER_API_KEY',
                'value': os.getenv('TOGETHER_API_KEY'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'development environment python',
        'previewEnabled': False,
        'applications': [engine,app],
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./llama_quickstart/src/services.py ###
import asyncio
from src.client import client
from src.functions.llm.chat import llm_chat
from src.functions.hn.search import hn_search
from src.workflows.workflow import HnWorkflow
from src.functions.crawl.website import crawl_website
from restack_ai.restack import ServiceOptions

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[HnWorkflow],
            functions=[hn_search, crawl_website]
        ),
        client.start_service(
            functions=[llm_chat],
            task_queue="llm_chat",
            options=ServiceOptions(
                rate_limit=1,
                max_concurrent_function_runs=1
            )
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./llama_quickstart/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)

client = Restack(connection_options)


### File: ./llama_quickstart/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.hn.search import hn_search
    from src.functions.hn.schema import HnSearchInput
    from src.functions.crawl.website import crawl_website
    from src.functions.llm.chat import llm_chat, FunctionInputParams

@workflow.defn()
class HnWorkflow:
    @workflow.run
    async def run(self, input: dict):

        query = input["query"]
        count = input["count"]
        hn_results = await workflow.step(hn_search, HnSearchInput(query=query, count=count), start_to_close_timeout=timedelta(seconds=10))
        urls = [hit['url'] for hit in hn_results['hits'] if 'url' in hit]

        crawled_contents = []
        for url in urls:  # Use the extracted URLs
            log.info("hn_result", extra={"url": url})
            if url:
                content = await workflow.step(crawl_website, url, start_to_close_timeout=timedelta(seconds=30))
                crawled_contents.append(content)
        
        summaries = []
        for content in crawled_contents:
            system_prompt = f"Provide a summary of the website for project found on Hacker news"
            user_prompt = f"Summarize the following content: {content}"
            summary = await workflow.step(llm_chat, FunctionInputParams(system_prompt=system_prompt, user_prompt=user_prompt), task_queue="llm_chat",start_to_close_timeout=timedelta(seconds=120))
            summaries.append(summary)

        system_prompt = f"You are a personal assistant. Provide a summary of the latest hacker news and the summaries of the websites. Structure your response with the title of the project, then a short description and a list of actionable bullet points."
        user_prompt = f"Here is the latest hacker news data: {str(hn_results)} and summaries of the websites: {str(summaries)}"

        return await workflow.step(llm_chat, FunctionInputParams(system_prompt=system_prompt,user_prompt=user_prompt), task_queue="llm_chat", start_to_close_timeout=timedelta(seconds=120))

### File: ./llama_quickstart/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from src.client import client
import time
import uvicorn


# Define request model
class QueryRequest(BaseModel):
    query: str
    count: int

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the TogetherAI LlamaIndex FastAPI App!"

@app.post("/api/schedule")
async def schedule_workflow(request: QueryRequest):
    try:

        workflow_id = f"{int(time.time() * 1000)}-LlmCompleteWorkflow"
        
        runId = await client.schedule_workflow(
            workflow_name="HnWorkflow",
            workflow_id=workflow_id,
            input={"query": request.query, "count": request.count}
        )
        print("Scheduled workflow", runId)
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {
            "result": result,
            "workflow_id": workflow_id,
            "run_id": runId
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# Remove Flask-specific run code since FastAPI uses uvicorn
def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./llama_quickstart/src/functions/llm/chat.py ###
from llama_index.llms.together import TogetherLLM
from restack_ai.function import function, log, FunctionFailure
from llama_index.core.llms import ChatMessage, MessageRole
import os
from pydantic import BaseModel
from dotenv import load_dotenv

load_dotenv()

class FunctionInputParams(BaseModel):
    system_prompt: str
    user_prompt: str

@function.defn()
async def llm_chat(input: FunctionInputParams):
    try:
        api_key = os.getenv("TOGETHER_API_KEY")
        if not api_key:
            log.error("TOGETHER_API_KEY environment variable is not set.")
            raise ValueError("TOGETHER_API_KEY environment variable is required.")
    
        llm = TogetherLLM(
            model="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo", api_key=api_key
        )
        messages = [
            ChatMessage(
                role=MessageRole.SYSTEM, content=input.system_prompt
            ),
            ChatMessage(
                role=MessageRole.USER, content=input.user_prompt
            ),
        ]
        resp = llm.chat(messages)
        return resp.message.content
    except Exception as e:
        log.error(f"Error interacting with llm: {e}")
        raise FunctionFailure(f"Error interacting with llm: {e}", non_retryable=True)
  

### File: ./llama_quickstart/src/functions/crawl/website.py ###
from restack_ai.function import function, log
import requests
from bs4 import BeautifulSoup

@function.defn()
async def crawl_website(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad responses

        # Parse the content with BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract the text content from the page
        content = soup.get_text()

        log.info("crawl_website", extra={"content": content})

        return content

    except requests.exceptions.RequestException as e:
        # Handle any exceptions that occur during the request
        log.error("crawl_website function failed", error=e)
        raise e


### File: ./llama_quickstart/src/functions/hn/search.py ###
import requests
from restack_ai.function import function, log
from src.functions.hn.schema import HnSearchInput

@function.defn()
async def hn_search(input: HnSearchInput):
    try:
        # Fetch the latest stories IDs
        response = requests.get(
            f"https://hn.algolia.com/api/v1/search_by_date?tags=show_hn&query={input.query}&hitsPerPage={input.count}&numericFilters=points>2"
        )
        data = response.json()

        log.info("hnSearch", extra={"data": data})
        return data
    except Exception as error:
        log.error("hn_search function failed", error=error)
        raise error

### File: ./llama_quickstart/src/functions/hn/schema.py ###
from pydantic import BaseModel, Field

class HnSearchInput(BaseModel):
    query: str = Field(default=None, description="The query for search")
    count: int = Field(default=5, description="The number of results to return")

### File: ./flask_togetherai_llamaindex/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def schedule_workflow(workflow_name):

    client = Restack()

    print(client)

    workflow_id = f"{int(time.time() * 1000)}-{workflow_name}"
    runId = await client.schedule_workflow(
        workflow_name=workflow_name,
        workflow_id=workflow_id,
        input='test'
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_llm_complete_workflow():
    asyncio.run(schedule_workflow("LlmCompleteWorkflow"))

if __name__ == "__main__":
    run_schedule_llm_complete_workflow()


### File: ./flask_togetherai_llamaindex/src/services.py ###
import asyncio
from src.functions.llm_complete import llm_complete
from src.client import restack_client
from src.workflows.workflow import LlmCompleteWorkflow
from dotenv import load_dotenv
load_dotenv()

async def main():
    await restack_client.start_service(
        workflows= [LlmCompleteWorkflow],
        functions= [llm_complete]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./flask_togetherai_llamaindex/src/client.py ###
from restack_ai import Restack

restack_client = Restack()

### File: ./flask_togetherai_llamaindex/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.llm_complete import llm_complete

@workflow.defn()
class LlmCompleteWorkflow:
    @workflow.run
    async def run(self, input: str):
        log.info("LlmCompleteWorkflow started", input=input)
        result = await workflow.step(llm_complete, input, start_to_close_timeout=timedelta(seconds=120))
        log.info("LlmCompleteWorkflow completed", result=result)
        return result


### File: ./flask_togetherai_llamaindex/src/app.py ###
import time
from flask import Flask, jsonify, request
from restack_ai import Restack
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

@app.route('/')
def home():
    return "Welcome to the TogetherAI LlamaIndex Flask App!"

# New endpoint to schedule workflow and get back result
@app.route('/api/schedule', methods=['POST'])
async def schedule_workflow():
    if request.is_json:
        
        prompt = request.json.get('prompt')
        client = Restack()

        workflow_id = f"{int(time.time() * 1000)}-LlmCompleteWorkflow"
        runId = await client.schedule_workflow(
            workflow_name="LlmCompleteWorkflow",
            workflow_id=workflow_id,
            input=prompt
        )

        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return jsonify({'result': result})

    else:
        return jsonify({'error': 'Request must be JSON'}), 400

def run_flask():
    app.run(debug=True) 

if __name__ == '__main__':
    run_flask() 

### File: ./flask_togetherai_llamaindex/src/functions/llm_complete.py ###
import os
from restack_ai.function import function, log
from llama_index.llms.together import TogetherLLM

@function.defn()
async def llm_complete(prompt) -> str:
    try:
        log.info("llm_complete function started", prompt=prompt)
        llm = TogetherLLM(
            model="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo", api_key=os.environ["TOGETHER_API_KEY"]
        )

        resp = llm.complete(prompt)

        log.info("llm_complete function completed", response=resp.text)

        return resp.text
    except Exception as e:
        log.error("llm_complete function failed", error=e)
        raise e
    

### File: ./fastapi_togetherai_llama/restack_up.py ###
import os
from restack_sdk_cloud import RestackCloud
from dotenv import load_dotenv  
load_dotenv()

async def main():
    # Initialize the RestackCloud client with the CLOUD token from environment variables
    restack_cloud_client = RestackCloud(os.getenv('RESTACK_CLOUD_TOKEN'))

    # Define the backend application configuration
    backend_app = {
        'name': 'backend-fastapi-togetherai-llama',
        'dockerFilePath': 'examples/fastapi_togetherai_llama/Dockerfile',
        'dockerBuildContext': 'examples/fastapi_togetherai_llama',
        'environmentVariables': [
            {
                'name': 'RESTACK_ENGINE_ID',
                'value': os.getenv('RESTACK_ENGINE_ID'),
            },
            {
                'name': 'RESTACK_ENGINE_ADDRESS',
                'value': os.getenv('RESTACK_ENGINE_ADDRESS'),
            },
            {
                'name': 'RESTACK_ENGINE_API_KEY',
                'value': os.getenv('RESTACK_ENGINE_API_KEY'),
            },
            {
                'name': 'TOGETHER_API_KEY',
                'value': os.getenv('TOGETHER_API_KEY'),
            },
        ],
    }

    # Configure the stack with the applications
    await restack_cloud_client.stack({
        'name': 'fastapi-togetherai-llama',
        'applications': [backend_app],
        'previewEnabled': False,
    })

    # Deploy the stack
    await restack_cloud_client.up()

# Run the main function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

### File: ./fastapi_togetherai_llama/src/services.py ###
import asyncio
from src.client import client
from src.functions.function import llm_complete
from src.workflows.workflow import LlmCompleteWorkflow
from restack_ai.restack import ServiceOptions

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[LlmCompleteWorkflow],
            functions=[llm_complete],
            options=ServiceOptions(
                rate_limit=1,
                max_concurrent_function_runs=1
            )
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./fastapi_togetherai_llama/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)

client = Restack(connection_options)

### File: ./fastapi_togetherai_llama/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.function import llm_complete, FunctionInputParams

@workflow.defn()
class LlmCompleteWorkflow:
    @workflow.run
    async def run(self, input: dict):
        log.info("LlmCompleteWorkflow started", input=input)
        prompt = input["prompt"]
        result = await workflow.step(llm_complete, FunctionInputParams(prompt=prompt), start_to_close_timeout=timedelta(seconds=120))
        log.info("LlmCompleteWorkflow completed", result=result)
        return result

### File: ./fastapi_togetherai_llama/src/app.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dataclasses import dataclass
import time
from src.client import client
import uvicorn

# Define request model
@dataclass
class PromptRequest:
    prompt: str

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return "Welcome to the TogetherAI LlamaIndex FastAPI App!"

@app.post("/api/schedule")
async def schedule_workflow(request: PromptRequest):
    try:
        workflow_id = f"{int(time.time() * 1000)}-LlmCompleteWorkflow"
        
        runId = await client.schedule_workflow(
            workflow_name="LlmCompleteWorkflow",
            workflow_id=workflow_id,
            input={"prompt": request.prompt}
        )
        
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )
        
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

def run_app():
    uvicorn.run("src.app:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == '__main__':
    run_app()


### File: ./fastapi_togetherai_llama/src/functions/function.py ###
from llama_index.llms.together import TogetherLLM
from restack_ai.function import function, log, FunctionFailure
from llama_index.core.llms import ChatMessage, MessageRole
import os
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class FunctionInputParams:
    prompt: str

@function.defn()
async def llm_complete(input: FunctionInputParams):
    try:
        log.info("llm_complete function started", input=input)
        llm = TogetherLLM(
            model="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo", api_key=os.getenv("TOGETHER_API_KEY")
        )
        messages = [
            ChatMessage(
                # This is a system prompt that is used to set the behavior of the LLM. You can update this llm_complete function to also accept a system prompt as an input parameter.
                role=MessageRole.SYSTEM, content="You are a pirate with a colorful personality"
            ),
            ChatMessage(role=MessageRole.USER, content=input.prompt),
        ]
        resp = llm.chat(messages)
        log.info("llm_complete function completed", response=resp.message.content)
        return resp.message.content
    except Exception as e:
        log.error(f"Error interacting with llm: {e}")
        raise FunctionFailure(f"Error interacting with llm: {e}", non_retryable=True)
  

### File: ./email_sender/schedule_workflow_failure.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class InputParams:
    email_context: str
    subject: str
    to: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-SendEmailWorkflow"
    to_email = os.getenv("TO_EMAIL")
    if not to_email:
        raise Exception("TO_EMAIL environment variable is not set")

    run_id = await client.schedule_workflow(
        workflow_name="SendEmailWorkflow",
        workflow_id=workflow_id,
        input={
            "email_context": "This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say 'Best regards' or something like that. No need to mention name of user or name of person sending the email.",
            "subject": "Hello from Restack",
            "to": to_email,
            "simulate_failure": True
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow_failure():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow_failure()


### File: ./email_sender/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class InputParams:
    email_context: str
    subject: str
    to: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-SendEmailWorkflow"
    to_email = os.getenv("TO_EMAIL")
    if not to_email:
        raise Exception("TO_EMAIL environment variable is not set")

    run_id = await client.schedule_workflow(
        workflow_name="SendEmailWorkflow",
        workflow_id=workflow_id,
        input={
            "email_context": "This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say 'Best regards' or something like that. No need to mention name of user or name of person sending the email.",
            "subject": "Hello from Restack",
            "to": to_email
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()


### File: ./email_sender/src/services.py ###
import asyncio
from src.client import client
from src.workflows.send_email import SendEmailWorkflow
from src.functions.send_email import send_email
from src.functions.generate_email_content import generate_email_content

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[SendEmailWorkflow],
            functions=[generate_email_content, send_email]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: ./email_sender/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)


### File: ./email_sender/src/workflows/send_email.py ###
from restack_ai.workflow import workflow, import_functions, log, RetryPolicy
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.send_email import send_email, SendEmailInput
    from src.functions.generate_email_content import generate_email_content, GenerateEmailInput

@dataclass
class WorkflowInputParams:
    email_context: str
    subject: str
    to: str
    simulate_failure: bool = False

@workflow.defn()
class SendEmailWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("SendEmailWorkflow started", input=input)

        text = await workflow.step(
            generate_email_content,
            GenerateEmailInput(
                email_context=input.email_context,
                simulate_failure=input.simulate_failure,
            ),
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=10),
                backoff_coefficient=1,
            ),
        )

        await workflow.step(
            send_email,
            SendEmailInput(
                text=text,
                subject=input.subject,
                to=input.to,
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        return 'Email sent successfully'


### File: ./email_sender/src/functions/generate_email_content.py ###
from restack_ai.function import function, log, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

tries = 0

@dataclass
class GenerateEmailInput:
    email_context: str
    simulate_failure: bool = False

@function.defn()
async def generate_email_content(input: GenerateEmailInput):
    global tries

    if input.simulate_failure and tries == 0:
        tries += 1
        raise FunctionFailure("Simulated failure", non_retryable=False)
    
    if (os.environ.get("OPENAI_API_KEY") is None):
        raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that generates short emails based on the provided context."
            },
            {
                "role": "user",
                "content": f"Generate a short email based on the following context: {input.email_context}"
            }
        ],
        max_tokens=150
    )
    
    return response.choices[0].message.content



### File: ./email_sender/src/functions/send_email.py ###
import sendgrid
from sendgrid.helpers.mail import Mail
import os
from restack_ai.function import function, FunctionFailure
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class SendEmailInput:
    text: str
    subject: str
    to: str

@function.defn()
async def send_email(input: SendEmailInput) -> None:
    from_email = os.getenv('FROM_EMAIL')
    
    if not from_email:
        raise FunctionFailure('FROM_EMAIL is not set', non_retryable=True)
    
    sendgrid_api_key = os.getenv('SENDGRID_API_KEY')

    if not sendgrid_api_key:
        raise FunctionFailure('SENDGRID_API_KEY is not set', non_retryable=True)
    
    
    message = Mail(
        from_email=from_email,
        to_emails=input.to,
        subject=input.subject,
        plain_text_content=input.text
    )
    
    try:
        sg = sendgrid.SendGridAPIClient(sendgrid_api_key)
        sg.send(message)
    except Exception:
        raise FunctionFailure('Failed to send email', non_retryable=False)


### File: ./child_workflows/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ParentWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="ParentWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./child_workflows/src/services.py ###
import asyncio
from src.functions.function import welcome
from src.client import client
from src.workflows.parent import ParentWorkflow
from src.workflows.child import ChildWorkflow

async def main():

    await client.start_service(
        workflows= [ParentWorkflow, ChildWorkflow],
        functions= [welcome]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./child_workflows/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: ./child_workflows/src/workflows/parent.py ###
from restack_ai.workflow import workflow, log, workflow_info

from .child import ChildWorkflow

@workflow.defn()
class ParentWorkflow:
    @workflow.run
    async def run(self):
        

        # use the parent run id to create child workflow ids

        parent_workflow_id = workflow_info().workflow_id

        log.info("Start ChildWorkflow and dont wait for result")

        result = await workflow.child_start(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-start")

        log.info("Start ChildWorkflow and wait for result")
        result = await workflow.child_execute(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-execute")
        log.info("ChildWorkflow completed", result=result)
        return result




### File: ./child_workflows/src/workflows/child.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome

@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self):
        log.info("ChildWorkflow started")
        result = await workflow.step(welcome, input="world", start_to_close_timeout=timedelta(seconds=120))
        log.info("ChildWorkflow completed", result=result)
        return result




### File: ./child_workflows/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn(name="welcome")
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: ./bostondynamics_spot/schedule_workflow.py ###
import asyncio
import time
from src.client import client

async def main():

    workflow_id = f"{int(time.time() * 1000)}-SpotWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="SpotWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: ./bostondynamics_spot/src/services.py ###
import asyncio
from src.client import client
from src.workflows.workflow import SpotWorkflow
async def main():

    await client.start_service(
        workflows= [SpotWorkflow]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./bostondynamics_spot/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)

### File: ./bostondynamics_spot/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.bostondynamics.commands import spot_commands

@workflow.defn()
class SpotWorkflow:
    @workflow.run
    async def run(self):
        log.info("SpotWorkflow started")
        result = await workflow.step(spot_commands, start_to_close_timeout=timedelta(seconds=120))
        log.info("SpotWorkflow completed", result=result)
        return result




### File: ./bostondynamics_spot/src/spot_services.py ###
import asyncio
from src.client import client
from src.functions.bostondynamics.commands import spot_commands
async def main():

    await client.start_service(
        functions= [spot_commands]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: ./bostondynamics_spot/src/functions/bostondynamics/spot_controller.py ###
import time
import bosdyn.client
from bosdyn.client.robot_command import RobotCommandClient, RobotCommandBuilder, blocking_stand  # , blocking_sit
from bosdyn.geometry import EulerZXY
from bosdyn.api.spot import robot_command_pb2 as spot_command_pb2
from bosdyn.client.frame_helpers import ODOM_FRAME_NAME
from bosdyn.api.basic_command_pb2 import RobotCommandFeedbackStatus
from bosdyn.client.estop import EstopClient, EstopEndpoint, EstopKeepAlive
from bosdyn.client.robot_state import RobotStateClient
from bosdyn.client.frame_helpers import ODOM_FRAME_NAME, VISION_FRAME_NAME, BODY_FRAME_NAME, \
    GRAV_ALIGNED_BODY_FRAME_NAME, get_se2_a_tform_b
from bosdyn.client import math_helpers

import traceback

VELOCITY_CMD_DURATION = 0.5


class SpotController:
    def __init__(self, username, password, robot_ip):
        self.username = username
        self.password = password
        self.robot_ip = robot_ip

        sdk = bosdyn.client.create_standard_sdk('ControllingSDK')

        self.robot = sdk.create_robot(robot_ip)
        id_client = self.robot.ensure_client('robot-id')

        self.robot.authenticate(username, password)
        self.command_client = self.robot.ensure_client(RobotCommandClient.default_service_name)
        self.robot.logger.info("Authenticated")

        self._lease_client = None
        self._lease = None
        self._lease_keepalive = None

        self._estop_client = self.robot.ensure_client(EstopClient.default_service_name)
        self._estop_endpoint = EstopEndpoint(self._estop_client, 'GNClient', 9.0)
        self._estop_keepalive = None

        self.state_client = self.robot.ensure_client(RobotStateClient.default_service_name)

    def release_estop(self):
        self._estop_endpoint.force_simple_setup()
        self._estop_keepalive = EstopKeepAlive(self._estop_endpoint)

    def set_estop(self):
        if self._estop_keepalive:
            try:
                self._estop_keepalive.stop()
            except:
                self.robot.logger.error("Failed to set estop")
                traceback.print_exc()
            self._estop_keepalive.shutdown()
            self._estop_keepalive = None

    def lease_control(self):
        self._lease_client = self.robot.ensure_client('lease')
        self._lease = self._lease_client.take()
        self._lease_keepalive = bosdyn.client.lease.LeaseKeepAlive(self._lease_client, must_acquire=True)
        self.robot.logger.info("Lease acquired")

    def return_lease(self):
        self._lease_client.return_lease(self._lease)
        self._lease_keepalive.shutdown()
        self._lease_keepalive = None

    def __enter__(self):
        self.lease_control()
        self.release_estop()
        self.power_on_stand_up()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            self.robot.logger.error("Spot powered off with " + exc_val + " exception")
        self.power_off_sit_down()
        self.return_lease()
        self.set_estop()

        return True if exc_type else False

    def move_head_in_points(self, yaws, pitches, rolls, body_height=0, sleep_after_point_reached=0, timeout=3):
        for i in range(len(yaws)):
            footprint_r_body = EulerZXY(yaw=yaws[i], roll=rolls[i], pitch=pitches[i])
            params = RobotCommandBuilder.mobility_params(footprint_R_body=footprint_r_body, body_height=body_height)
            blocking_stand(self.command_client, timeout_sec=timeout, update_frequency=0.02, params=params)
            self.robot.logger.info("Moved to yaw={} rolls={} pitch={}".format(yaws[i], rolls[i], pitches[i]))
            if sleep_after_point_reached:
                time.sleep(sleep_after_point_reached)

    def wait_until_action_complete(self, cmd_id, timeout=15):
        start_time = time.time()
        while time.time() - start_time < timeout:
            feedback = self.command_client.robot_command_feedback(cmd_id)
            mobility_feedback = feedback.feedback.synchronized_feedback.mobility_command_feedback
            if mobility_feedback.status != RobotCommandFeedbackStatus.STATUS_PROCESSING:
                print("Failed to reach the goal")
                return False
            traj_feedback = mobility_feedback.se2_trajectory_feedback
            if (traj_feedback.status == traj_feedback.STATUS_AT_GOAL and
                    traj_feedback.body_movement_status == traj_feedback.BODY_STATUS_SETTLED):
                print("Arrived at the goal.")
                return True
            time.sleep(0.5)

    def move_to_goal(self, goal_x=0, goal_y=0):
        cmd = RobotCommandBuilder.synchro_trajectory_command_in_body_frame(
            goal_x_rt_body=goal_x,
            goal_y_rt_body=goal_y,
            goal_heading_rt_body=0,
            frame_tree_snapshot=self.robot.get_frame_tree_snapshot()
        )
        # cmd = RobotCommandBuilder.synchro_se2_trajectory_point_command(goal_x=goal_x, goal_y=goal_y, goal_heading=0,
        #                                                                frame_name=GRAV_ALIGNED_BODY_FRAME_NAME)
        cmd_id = self.command_client.robot_command(lease=None, command=cmd,
                                                   end_time_secs=time.time() + 10)
        self.wait_until_action_complete(cmd_id)

        self.robot.logger.info("Moved to x={} y={}".format(goal_x, goal_y))

    def power_on_stand_up(self):
        self.robot.power_on(timeout_sec=20)
        assert self.robot.is_powered_on(), "Not powered on"
        self.robot.time_sync.wait_for_sync()
        blocking_stand(self.command_client, timeout_sec=10)

    def power_off_sit_down(self):
        self.move_head_in_points(yaws=[0], pitches=[0], rolls=[0])
        self.robot.power_off(cut_immediately=False)

    def make_stance(self, x_offset, y_offset):
        state = self.state_client.get_robot_state()
        vo_T_body = get_se2_a_tform_b(state.kinematic_state.transforms_snapshot,
                                      VISION_FRAME_NAME,
                                      GRAV_ALIGNED_BODY_FRAME_NAME)

        pos_fl_rt_vision = vo_T_body * math_helpers.SE2Pose(x_offset, y_offset, 0)
        pos_fr_rt_vision = vo_T_body * math_helpers.SE2Pose(x_offset, -y_offset, 0)
        pos_hl_rt_vision = vo_T_body * math_helpers.SE2Pose(-x_offset, y_offset, 0)
        pos_hr_rt_vision = vo_T_body * math_helpers.SE2Pose(-x_offset, -y_offset, 0)

        stance_cmd = RobotCommandBuilder.stance_command(
            VISION_FRAME_NAME, pos_fl_rt_vision.position,
            pos_fr_rt_vision.position, pos_hl_rt_vision.position, pos_hr_rt_vision.position)

        start_time = time.time()
        while time.time() - start_time < 6:
            # Update end time
            stance_cmd.synchronized_command.mobility_command.stance_request.end_time.CopyFrom(
                self.robot.time_sync.robot_timestamp_from_local_secs(time.time() + 5))
            # Send the command
            self.command_client.robot_command(stance_cmd)
            time.sleep(0.1)

    def move_by_velocity_control(self, v_x=0.0, v_y=0.0, v_rot=0.0, cmd_duration=VELOCITY_CMD_DURATION):
        # v_x+ - forward, v_y+ - left | m/s, v_rot+ - counterclockwise |rad/s
        self._start_robot_command(
            RobotCommandBuilder.synchro_velocity_command(v_x=v_x, v_y=v_y, v_rot=v_rot),
            end_time_secs=time.time() + cmd_duration)

    def _start_robot_command(self, command_proto, end_time_secs=None):
        self.command_client.robot_command(lease=None, command=command_proto, end_time_secs=end_time_secs)

    def stand_at_height(self, body_height):
        cmd = RobotCommandBuilder.synchro_stand_command(body_height=body_height)
        self.command_client.robot_command(cmd)

    def bow(self, pitch, body_height=0, sleep_after_point_reached=0):
        self.move_head_in_points([0, 0], [pitch, 0], [0, 0], body_height=body_height,
                                 sleep_after_point_reached=sleep_after_point_reached, timeout=3)

    def dust_off(self, yaws, pitches, rolls):
        self.move_head_in_points(yaws, pitches, rolls, sleep_after_point_reached=0, body_height=0)


### File: ./bostondynamics_spot/src/functions/bostondynamics/commands.py ###
import os
import time
from .spot_controller import SpotController
import cv2
from dotenv import load_dotenv  
from restack_ai.function import function, log

load_dotenv()

def capture_image():
    camera_capture = cv2.VideoCapture(0)
    rv, image = camera_capture.read()
    print(f"Image Dimensions: {image.shape}")
    camera_capture.release()

@function.defn()
async def spot_commands():
    #example of using micro and speakers
    print("Start recording audio")
    sample_name = "aaaa.wav"
    cmd = f'arecord -vv --format=cd --device={os.environ["AUDIO_INPUT_DEVICE"]} -r 48000 --duration=10 -c 1 {sample_name}'
    print(cmd)
    os.system(cmd)
    print("Playing sound")
    os.system(f"ffplay -nodisp -autoexit -loglevel quiet {sample_name}")


    ROBOT_IP = os.getenv['ROBOT_IP']
    SPOT_USERNAME = os.getenv['SPOT_USERNAME']
    SPOT_PASSWORD = os.getenv['SPOT_PASSWORD']

    # # Capture image

    # Use wrapper in context manager to lease control, turn on E-Stop, power on the robot and stand up at start
    # and to return lease + sit down at the end
    with SpotController(username=SPOT_USERNAME, password=SPOT_PASSWORD, robot_ip=ROBOT_IP) as spot:

        time.sleep(2)
        capture_image()
        # Move head to specified positions with intermediate time.sleep
        spot.move_head_in_points(yaws=[0.2, 0],
                                 pitches=[0.3, 0],
                                 rolls=[0.4, 0],
                                 sleep_after_point_reached=1)
        capture_image()
        time.sleep(3)

        # Make Spot to move by goal_x meters forward and goal_y meters left
        spot.move_to_goal(goal_x=0.5, goal_y=0)
        time.sleep(3)
        capture_image()

        # Control Spot by velocity in m/s (or in rad/s for rotation)
        spot.move_by_velocity_control(v_x=-0.3, v_y=0, v_rot=0, cmd_duration=2)
        capture_image()
        time.sleep(3)



### File: ./bostondynamics_spot/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn()
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


